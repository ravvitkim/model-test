"""
ë¬¸ì„œ ë¡œë” v7.0 - ì™„ì „ ë¦¬íŒ©í† ë§

ğŸ”¥ í•µì‹¬ ê°œì„ :
1. ë¬¸ì„œ í˜•ì‹ ìë™ ê°ì§€ (ìˆ«ìí˜• vs ì´ë¦„í˜•)
2. ëª©ì°¨ ê°ì§€ ë° ìŠ¤í‚µ
3. ì •ê·œí™” íŒŒì´í”„ë¼ì¸
4. ê³„ì¸µ ìŠ¤íƒ ë‹¨ìˆœí™”
"""

import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from io import BytesIO


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class ContentBlock:
    """ë¬¸ì„œ ë¸”ë¡"""
    text: str
    block_type: str = "text"
    section: Optional[str] = None
    page: Optional[int] = None
    metadata: Dict = field(default_factory=dict)


@dataclass 
class ParsedDocument:
    """íŒŒì‹±ëœ ë¬¸ì„œ"""
    text: str
    blocks: List[ContentBlock]
    metadata: Dict
    tables: List[Dict] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ìŠ¤íŠ¸ ì •ê·œí™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ê·œí™”
    - ì „ê° â†’ ë°˜ê°
    - ë¡œë§ˆ ìˆ«ì â†’ ì•„ë¼ë¹„ì•„ ìˆ«ì
    - ì„¹ì…˜ ë²ˆí˜¸ í˜•ì‹ í†µì¼
    """
    # ë¡œë§ˆ ìˆ«ì ë³€í™˜
    roman_map = {
        'â… ': '1', 'â…¡': '2', 'â…¢': '3', 'â…£': '4', 'â…¤': '5',
        'â…¥': '6', 'â…¦': '7', 'â…§': '8', 'â…¨': '9', 'â…©': '10',
        'â…°': '1', 'â…±': '2', 'â…²': '3', 'â…³': '4', 'â…´': '5',
    }
    for roman, arabic in roman_map.items():
        text = text.replace(roman, arabic)
    
    # ì „ê° â†’ ë°˜ê°
    text = text.replace('ï¼', '.').replace('ï¼', '-').replace('ã€€', ' ')
    text = text.replace('ï¼š', ':').replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    
    # í•˜ì´í”ˆ â†’ ì  (5-1 â†’ 5.1)
    text = re.sub(r'(\d+)\s*[-â€â€‘â€“â€”]\s*(\d+)', r'\1.\2', text)
    
    # ìˆ«ì.ìˆ«ì ê³µë°± ì œê±° (5 . 1 â†’ 5.1)
    text = re.sub(r'(\d+)\s*\.\s*(\d+)', r'\1.\2', text)
    
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¹ì…˜ íŒ¨í„´ ë§¤ì¹­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ (í•œê¸€ + ì˜ë¬¸)
SECTION_KEYWORDS = {
    'ëª©ì ': 'Purpose',
    'ì ìš©ë²”ìœ„': 'Scope',
    'ì ìš© ë²”ìœ„': 'Scope',
    'ì •ì˜': 'Definitions',
    'ì±…ì„': 'Responsibilities',
    'ì ˆì°¨': 'Procedure',
    'ì°¸ê³ ë¬¸í—Œ': 'Reference',
    'ì²¨ë¶€': 'Attachments',
    'ê¸°íƒ€': 'Others',
    'ëª©ì°¨': 'Table of Contents',
}


def detect_section(line: str) -> Optional[Dict]:
    """
    ë¼ì¸ì—ì„œ ì„¹ì…˜ ì •ë³´ ì¶”ì¶œ
    
    Returns:
        {
            "num": "5.1" ë˜ëŠ” "ëª©ì ",
            "type": "section" | "subsection" | "subsubsection" | "named_section" | "toc",
            "title": "ì œí’ˆí‘œì¤€ì„œ ë²ˆí˜¸ ì²´ê³„...",
            "level": 1 | 2 | 3  # ê³„ì¸µ ë ˆë²¨
        }
    """
    line = normalize_text(line.strip())
    if not line:
        return None
    
    # 1ï¸âƒ£ ëª©ì°¨ ê°ì§€
    if line.startswith('ëª©ì°¨') or line.lower().startswith('table of contents'):
        return {"num": "TOC", "type": "toc", "title": "", "level": 0}
    
    # 2ï¸âƒ£ ìˆ«ìí˜• ì„¹ì…˜ (ê°€ì¥ êµ¬ì²´ì ì¸ ê²ƒë¶€í„°!)
    patterns = [
        # 5.1.1 í˜•ì‹
        (r'^(\d+\.\d+\.\d+)\s+(.+)', 'subsubsection', 3),
        # 5.1 í˜•ì‹  
        (r'^(\d+\.\d+)\s+(.+)', 'subsection', 2),
        # 5. í˜•ì‹ (ì  ìˆìŒ)
        (r'^(\d+)\.\s+(.+)', 'section', 1),
        # 5 xxx í˜•ì‹ (ì  ì—†ìŒ, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        (r'^(\d+)\s+([ê°€-í£A-Za-z].+)', 'section', 1),
        # ì œNì¡°, ì œNì¥
        (r'^ì œ\s*(\d+)\s*ì¡°\s*(.*)', 'article', 1),
        (r'^ì œ\s*(\d+)\s*ì¥\s*(.*)', 'chapter', 1),
        (r'^ì œ\s*(\d+)\s*ë ˆë²¨\s*[:\(]?\s*(.+)', 'level', 2),
    ]
    
    for pattern, sec_type, level in patterns:
        m = re.match(pattern, line)
        if m:
            num = m.group(1)
            title = m.group(2).strip() if m.lastindex >= 2 else ""
            return {"num": num, "type": sec_type, "title": title, "level": level}
    
    # 3ï¸âƒ£ ì´ë¦„í˜• ì„¹ì…˜ (ìˆ«ìë¡œ ì‹œì‘í•˜ì§€ ì•Šì„ ë•Œë§Œ!)
    if re.match(r'^\d', line):
        return None  # ìˆ«ìë¡œ ì‹œì‘í•˜ë©´ ì´ë¦„í˜• ì•„ë‹˜
    
    # ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ
    for keyword, eng in SECTION_KEYWORDS.items():
        if keyword == 'ëª©ì°¨':
            continue  # ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬
        
        # "ëª©ì  Purpose" ë˜ëŠ” "ëª©ì " í˜•ì‹
        pattern = rf'^{re.escape(keyword)}\s*({eng})?'
        if re.match(pattern, line, re.IGNORECASE):
            return {"num": keyword, "type": "named_section", "title": eng, "level": 1}
    
    # 4ï¸âƒ£ ì†Œì œëª© ê°ì§€ (í•œê¸€ ë˜ëŠ” ì˜ë¬¸ + ì˜ë¬¸ ê´„í˜¸ë¡œ ëë‚˜ëŠ” ê²½ìš°)
    # ì˜ˆ: "ì œí’ˆí‘œì¤€ì„œ ë²ˆí˜¸ ì²´ê³„ ë° ë¬¸ì„œ ìœ í˜• (Numbering & Document Type)"
    # ì˜ˆ: "ê²€í†  ë° ìŠ¹ì¸ (Review & Approval)"
    # ì˜ˆ: "ì œì •(ì‘ì„±) ë° ë“±ë¡ (Creation & Registration)"
    # ì˜ˆ: "EDMS ê³„ì • ë° ê¶Œí•œê´€ë¦¬ (Account & Role Management)"
    
    # íŒ¨í„´ 1: í•œê¸€ë¡œ ì‹œì‘
    subtitle_pattern1 = r'^([ê°€-í£][ê°€-í£\s\(\)/Â·\-]+)\s*\(([A-Za-z\s&/\-:]+)\)\s*$'
    m = re.match(subtitle_pattern1, line)
    if m:
        korean_title = m.group(1).strip()
        # í•œê¸€ ì œëª©ì—ì„œ ê´„í˜¸ ë‚´ìš© ì œê±°
        korean_title = re.sub(r'\([^)]*\)', '', korean_title).strip()
        english_title = m.group(2).strip()
        return {
            "num": korean_title[:20],
            "type": "subsection",
            "title": english_title,
            "level": 2
        }
    
    # íŒ¨í„´ 2: ì˜ë¬¸ìœ¼ë¡œ ì‹œì‘ (EDMS, GMP ë“±)
    subtitle_pattern2 = r'^([A-Z][A-Za-z]*\s+[ê°€-í£][ê°€-í£\s\(\)/Â·\-]+)\s*\(([A-Za-z\s&/\-:]+)\)\s*$'
    m = re.match(subtitle_pattern2, line)
    if m:
        korean_title = m.group(1).strip()
        korean_title = re.sub(r'\([^)]*\)', '', korean_title).strip()
        english_title = m.group(2).strip()
        return {
            "num": korean_title[:25],
            "type": "subsection",
            "title": english_title,
            "level": 2
        }
    
    return None


def detect_document_format(lines: List[str]) -> str:
    """
    ë¬¸ì„œ í˜•ì‹ ê°ì§€
    
    Returns:
        "numbered": ìˆ«ìí˜• (1 ëª©ì , 5.1 xxx)
        "named": ì´ë¦„í˜• (ëª©ì  Purpose, ì ˆì°¨ Procedure)
    """
    numbered_count = 0
    named_count = 0
    
    for line in lines[:50]:  # ì²« 50ì¤„ë§Œ ê²€ì‚¬
        line = normalize_text(line.strip())
        if not line:
            continue
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜
        if re.match(r'^\d+[\.\s]', line):
            numbered_count += 1
        
        # ì´ë¦„í˜• ì„¹ì…˜
        for keyword in SECTION_KEYWORDS:
            if line.startswith(keyword):
                named_count += 1
                break
    
    return "numbered" if numbered_count > named_count else "named"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¸”ë¡ ì¶”ì¶œ (í•µì‹¬ ë¡œì§)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_blocks(text: str) -> List[ContentBlock]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ ë¸”ë¡ ì¶”ì¶œ
    
    ğŸ”¥ í•µì‹¬ ë¡œì§:
    1. ë¬¸ì„œ í˜•ì‹ ê°ì§€ (ìˆ«ìí˜• vs ì´ë¦„í˜•)
    2. ëª©ì°¨ ê°ì§€ ë° ìŠ¤í‚µ
    3. ì„¹ì…˜ ê³„ì¸µ ì¶”ì  (ìŠ¤íƒ)
    4. section_path ìƒì„±
    """
    lines = text.split('\n')
    doc_format = detect_document_format(lines)
    
    blocks = []
    current_lines = []
    current_meta = {"num": None, "type": "intro", "title": "", "level": 0}
    
    # ê³„ì¸µ ìŠ¤íƒ: [{"num": "5", "title": "ì ˆì°¨"}, {"num": "5.1", "title": "xxx"}, ...]
    stack = []
    
    # ëª©ì°¨ ì˜ì—­ ì¶”ì 
    in_toc = False
    toc_end_patterns = ['ëª©ì ', 'Purpose', '1 ', '1.']
    
    # SOP ID ì¶”ì¶œ
    sop_id = ""
    sop_pattern = re.compile(r'((?:EQ-)?SOP[-_]?\d{4,5})', re.IGNORECASE)
    
    def build_section_path() -> Tuple[str, str]:
        """ìŠ¤íƒì—ì„œ section_path ìƒì„±"""
        if not stack:
            return (None, None)
        
        path_parts = [s["num"] for s in stack]
        readable_parts = []
        for s in stack:
            if s["title"]:
                readable_parts.append(f"{s['num']} {s['title']}")
            else:
                readable_parts.append(str(s["num"]))
        
        return (" > ".join(path_parts), " > ".join(readable_parts))
    
    def flush():
        nonlocal current_lines, current_meta
        if current_lines:
            block_text = '\n'.join(current_lines).strip()
            if block_text:
                path, path_readable = build_section_path()
                
                blocks.append(ContentBlock(
                    text=block_text,
                    block_type=current_meta["type"],
                    section=current_meta["num"],
                    metadata={
                        "article_num": current_meta["num"],
                        "article_type": current_meta["type"],
                        "title": current_meta.get("title", ""),
                        "sop_id": sop_id,
                        "section_path": path,
                        "section_path_readable": path_readable,
                    }
                ))
        current_lines = []
    
    for line in lines:
        line_strip = line.strip()
        
        # SOP ID ì¶”ì¶œ
        sop_match = sop_pattern.search(line_strip)
        if sop_match and not sop_id:
            sop_id = sop_match.group(1).upper().replace('_', '-')
            if not sop_id.startswith('EQ-'):
                sop_id = 'EQ-' + sop_id
        
        # ë¹ˆ ì¤„
        if not line_strip:
            current_lines.append(line)
            continue
        
        # ì„¹ì…˜ ê°ì§€
        section_info = detect_section(line_strip)
        
        # ëª©ì°¨ ì²˜ë¦¬
        if section_info and section_info["type"] == "toc":
            in_toc = True
            flush()
            current_lines = [line]
            current_meta = {"num": "TOC", "type": "toc", "title": "", "level": 0}
            continue
        
        # ëª©ì°¨ ì¢…ë£Œ ê°ì§€
        if in_toc:
            for pattern in toc_end_patterns:
                if line_strip.startswith(pattern) and section_info:
                    in_toc = False
                    break
            
            if in_toc:
                current_lines.append(line)
                continue
        
        # ìƒˆ ì„¹ì…˜ ì‹œì‘
        if section_info:
            flush()
            current_lines = [line]
            
            level = section_info["level"]
            
            # ìŠ¤íƒ ì—…ë°ì´íŠ¸
            # í˜„ì¬ ë ˆë²¨ë³´ë‹¤ ê°™ê±°ë‚˜ ë‚®ì€ í•­ëª© ì œê±°
            while stack and stack[-1].get("level", 0) >= level:
                stack.pop()
            
            # í˜„ì¬ ì„¹ì…˜ ì¶”ê°€
            stack.append({
                "num": section_info["num"],
                "title": section_info["title"],
                "level": level
            })
            
            current_meta = section_info
        else:
            current_lines.append(line)
    
    flush()
    return blocks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DOCX íŒŒì‹±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_docx(filename: str, content: bytes) -> ParsedDocument:
    """DOCX ë¬¸ì„œ íŒŒì‹±"""
    from docx import Document
    from docx.table import Table
    
    doc = Document(BytesIO(content))
    
    # í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ì¶”ì¶œ (ìˆœì„œëŒ€ë¡œ)
    full_text_parts = []
    tables_data = []
    
    # 1. ë¬¸ë‹¨ ì¶”ì¶œ
    for para in doc.paragraphs:
        text = para.text.strip()
        if text:
            full_text_parts.append(text)
    
    # 2. í…Œì´ë¸” ì¶”ì¶œ (í…ìŠ¤íŠ¸ ëì— ì¶”ê°€)
    for table in doc.tables:
        table_text, table_data = _parse_table(table)
        if table_text:
            full_text_parts.append(table_text)
        if table_data:
            tables_data.append(table_data)
    
    full_text = '\n'.join(full_text_parts)
    
    # ë¸”ë¡ ì¶”ì¶œ
    blocks = extract_blocks(full_text)
    
    # ë©”íƒ€ë°ì´í„°
    metadata = {
        "file_name": filename,
        "file_type": ".docx",
        "title": _extract_title(full_text),
        "parser": "docx_v7"
    }
    metadata.update(_extract_sop_metadata(full_text))
    
    return ParsedDocument(
        text=full_text,
        blocks=blocks,
        metadata=metadata,
        tables=tables_data
    )


def _parse_table(table) -> Tuple[str, Dict]:
    """í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ì™€ êµ¬ì¡°í™” ë°ì´í„°ë¡œ ë³€í™˜"""
    rows = []
    for row in table.rows:
        cells = [cell.text.strip() for cell in row.cells]
        rows.append(cells)
    
    if not rows:
        return "", {}
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    md_lines = []
    if rows:
        # í—¤ë”
        md_lines.append("| " + " | ".join(rows[0]) + " |")
        md_lines.append("| " + " | ".join(["---"] * len(rows[0])) + " |")
        # ë³¸ë¬¸
        for row in rows[1:]:
            # ì…€ ê°œìˆ˜ ë§ì¶”ê¸°
            while len(row) < len(rows[0]):
                row.append("")
            md_lines.append("| " + " | ".join(row[:len(rows[0])]) + " |")
    
    table_text = '\n'.join(md_lines)
    table_data = {"rows": rows, "markdown": table_text}
    
    return table_text, table_data


def _extract_title(text: str) -> str:
    """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
    lines = [l.strip() for l in text.split('\n') if l.strip()][:10]
    
    for line in lines:
        # SOP ì œëª© íŒ¨í„´
        if 'SOP' in line.upper() or 'ê¸°ì¤€ì„œ' in line or 'ê·œì •' in line:
            return line[:100]
    
    return lines[0][:100] if lines else "ë¬¸ì„œ"


def _extract_sop_metadata(text: str) -> Dict:
    """SOP ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {}
    
    # SOP ID
    sop_match = re.search(r'((?:EQ-)?SOP[-_]?\d{4,5})', text, re.IGNORECASE)
    if sop_match:
        sop_id = sop_match.group(1).upper().replace('_', '-')
        if not sop_id.startswith('EQ-'):
            sop_id = 'EQ-' + sop_id
        metadata["sop_id"] = sop_id
    
    # ë²„ì „
    ver_match = re.search(r'(?:ë²„ì „|Version|Rev\.?)\s*[:.]?\s*(\d+(?:\.\d+)?)', text, re.IGNORECASE)
    if ver_match:
        metadata["version"] = ver_match.group(1)
    
    return metadata


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ë¡œë“œ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_document(filename: str, content: bytes) -> ParsedDocument:
    """
    ë¬¸ì„œ ë¡œë“œ ë©”ì¸ í•¨ìˆ˜
    
    ì§€ì› í˜•ì‹: .docx, .doc, .pdf, .txt, .md, .html
    """
    # í™•ì¥ì ì¶”ì¶œ (íŒŒì¼ëª…ì— íŠ¹ìˆ˜ ë¬¸ìê°€ ìˆì„ ìˆ˜ ìˆìŒ)
    filename_lower = filename.lower()
    
    # ì‹¤ì œ í™•ì¥ì ê°ì§€
    if '.docx' in filename_lower:
        ext = '.docx'
    elif '.doc' in filename_lower:
        ext = '.doc'
    elif '.pdf' in filename_lower:
        ext = '.pdf'
    elif '.txt' in filename_lower:
        ext = '.txt'
    elif '.md' in filename_lower:
        ext = '.md'
    elif '.html' in filename_lower or '.htm' in filename_lower:
        ext = '.html'
    else:
        ext = Path(filename).suffix.lower()
    
    if ext in [".docx", ".doc"]:
        return load_docx(filename, content)
    
    if ext in [".txt", ".md"]:
        text = content.decode('utf-8', errors='ignore')
        blocks = extract_blocks(text)
        metadata = {
            "file_name": filename,
            "file_type": ext,
            "title": _extract_title(text),
            "parser": "text_v7"
        }
        metadata.update(_extract_sop_metadata(text))
        return ParsedDocument(text=text, blocks=blocks, metadata=metadata)
    
    if ext == ".pdf":
        return _load_pdf(filename, content)
    
    if ext in [".html", ".htm"]:
        return _load_html(filename, content)
    
    # ê¸°ë³¸: í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
    text = content.decode('utf-8', errors='ignore')
    blocks = extract_blocks(text)
    return ParsedDocument(
        text=text,
        blocks=blocks,
        metadata={"file_name": filename, "file_type": ext, "parser": "fallback"}
    )


def _load_pdf(filename: str, content: bytes) -> ParsedDocument:
    """PDF ë¡œë“œ (Docling ë˜ëŠ” fallback)"""
    try:
        from docling.document_converter import DocumentConverter
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as f:
            f.write(content)
            temp_path = f.name
        
        try:
            converter = DocumentConverter()
            result = converter.convert(temp_path)
            text = result.document.export_to_markdown()
        finally:
            os.unlink(temp_path)
        
        blocks = extract_blocks(text)
        metadata = {
            "file_name": filename,
            "file_type": ".pdf",
            "title": _extract_title(text),
            "parser": "docling"
        }
        metadata.update(_extract_sop_metadata(text))
        return ParsedDocument(text=text, blocks=blocks, metadata=metadata)
        
    except ImportError:
        # PyPDF2 fallback
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(BytesIO(content))
            text = '\n'.join(page.extract_text() or '' for page in reader.pages)
            blocks = extract_blocks(text)
            return ParsedDocument(
                text=text,
                blocks=blocks,
                metadata={"file_name": filename, "file_type": ".pdf", "parser": "pypdf2"}
            )
        except:
            return ParsedDocument(
                text="[PDF íŒŒì‹± ì‹¤íŒ¨]",
                blocks=[],
                metadata={"file_name": filename, "file_type": ".pdf", "parser": "failed"}
            )


def _load_html(filename: str, content: bytes) -> ParsedDocument:
    """HTML ë¡œë“œ"""
    from bs4 import BeautifulSoup
    
    html_text = content.decode('utf-8', errors='ignore')
    soup = BeautifulSoup(html_text, 'html.parser')
    
    # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    text = soup.get_text(separator='\n', strip=True)
    blocks = extract_blocks(text)
    
    title = soup.title.string if soup.title else _extract_title(text)
    
    return ParsedDocument(
        text=text,
        blocks=blocks,
        metadata={"file_name": filename, "file_type": ".html", "title": title, "parser": "bs4"}
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_supported_extensions() -> List[str]:
    """ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì"""
    return [".docx", ".doc", ".pdf", ".txt", ".md", ".html", ".htm"]


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    test_text = """
ëª©ì°¨ Table of Contents
1 ëª©ì  Purpose
2 ì ìš© ë²”ìœ„ Scope
5 ì ˆì°¨ Procedure
5.1 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ êµ¬ì„± ë° ê´€ë¦¬
5.1.1 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œ ë¬¸ì„œë²ˆí˜¸ëŠ”...

ëª©ì  Purpose
ë³¸ ê¸°ì¤€ì„œëŠ” í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ ì‘ì„±, ê²€í† , ìŠ¹ì¸ì— ê´€í•œ ê¸°ì¤€ì„ ì •í•œë‹¤.

ì ìš© ë²”ìœ„ Scope
ë³¸ ê¸°ì¤€ì„œëŠ” íšŒì‚¬ ë‚´ í’ˆì§ˆê´€ë¦¬ í™œë™ ì „ë°˜ì— ì ìš©ëœë‹¤.

ì ˆì°¨ Procedure
í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ êµ¬ì„± ë° ê´€ë¦¬
í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œëŠ” ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œë‹¤.

5.1 í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œì˜ êµ¬ì„± ë° ê´€ë¦¬
í’ˆì§ˆê´€ë¦¬ê¸°ì¤€ì„œëŠ” ì‹œí—˜ë°©ë²•, ê·œê²© ë“±ì„ ì •ì˜í•œë‹¤.

5.1.1 ë¬¸ì„œë²ˆí˜¸ ì²´ê³„
ë¬¸ì„œë²ˆí˜¸ëŠ” EQ-SOP-XXXXX í˜•ì‹ì„ ë”°ë¥¸ë‹¤.

5.1.2 ê°œì • ê´€ë¦¬
ê°œì • ì‹œ ë³€ê²½ ì´ë ¥ì„ ê¸°ë¡í•œë‹¤.
"""
    
    blocks = extract_blocks(test_text)
    
    print("=" * 60)
    print("ë¸”ë¡ ì¶”ì¶œ ê²°ê³¼")
    print("=" * 60)
    
    for i, block in enumerate(blocks):
        print(f"\n[{i}] type={block.metadata.get('article_type')}")
        print(f"    num={block.metadata.get('article_num')}")
        print(f"    path={block.metadata.get('section_path_readable')}")
        print(f"    text={block.text[:50]}...")