"""
ë¬¸ì„œ ë¡œë” v6.2 - section_path ê³„ì¸µ ì¶”ì  ì¶”ê°€
- PDF, DOCX, HTML, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
- í‘œ(Table) íŒŒì‹± ì§€ì›
- section_path: "5 > 5.1 > 5.1.1" í˜•íƒœì˜ ê³„ì¸µ ê²½ë¡œ
- section_path_readable: "5 ì ˆì°¨ > 5.1 ë¬¸ì„œì²´ê³„ > 5.1.1 Level 1" í˜•íƒœ
"""

import re
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
import io
import tempfile
import os


@dataclass
class ContentBlock:
    """ë¬¸ì„œì˜ ì˜ë¯¸ ë‹¨ìœ„ ë¸”ë¡"""
    text: str
    block_type: str  # title, paragraph, table, list, article ë“±
    level: int = 0
    page: Optional[int] = None
    section: Optional[str] = None
    metadata: Dict = field(default_factory=dict)


@dataclass
class ParsedDocument:
    """íŒŒì‹±ëœ ë¬¸ì„œ"""
    text: str
    blocks: List[ContentBlock]
    metadata: Dict
    tables: List[Dict] = field(default_factory=list)  # í‘œ ë°ì´í„°


def get_supported_extensions() -> list:
    """ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì"""
    return [".txt", ".md", ".pdf", ".docx", ".doc", ".html", ".htm", ".csv", ".xlsx", ".pptx", ".png", ".jpg", ".jpeg"]


def load_document(filename: str, content: bytes) -> ParsedDocument:
    """
    ë¬¸ì„œ ë¡œë“œ ë° íŒŒì‹± (ë©”ì¸ ì§„ì…ì )
    
    Returns:
        ParsedDocument: íŒŒì‹±ëœ ë¬¸ì„œ (text + blocks + metadata + tables)
    """
    ext = Path(filename).suffix.lower()

    # DOCX: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (Docling í‘œ + python-docx í…ìŠ¤íŠ¸)
    if ext in [".docx", ".doc"]:
        return _load_docx_hybrid(filename, content)

    # PDF, PPTX, XLSX, HTML, ì´ë¯¸ì§€ â†’ Docling ì‹œë„
    if ext in [".pdf", ".pptx", ".xlsx", ".html", ".htm", ".png", ".jpg", ".jpeg"]:
        try:
            return _load_with_docling(filename, content, ext)
        except ImportError:
            print("âš ï¸ Docling not installed, falling back to basic parser")
            if ext == ".pdf":
                return _load_pdf_basic(filename, content)
            elif ext in [".html", ".htm"]:
                return _load_html_basic(filename, content)
        except Exception as e:
            print(f"âš ï¸ Docling failed: {e}, falling back to basic parser")
            if ext == ".pdf":
                return _load_pdf_basic(filename, content)
            elif ext in [".html", ".htm"]:
                return _load_html_basic(filename, content)

    # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼
    if ext in [".txt", ".md"]:
        text = _decode_text(content)
        if _is_article_document(text):
            return _parse_articles(text, filename, ext)
        return _parse_plain_text(text, filename, ext)

    if ext == ".csv":
        return _load_csv(filename, content)

    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    text = _decode_text(content)
    return _parse_plain_text(text, filename, ext)


def _load_docx_hybrid(filename: str, content: bytes) -> ParsedDocument:
    """
    DOCX í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±
    - ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤‘ìš”!)
    - í‘œ(Table)ë¥¼ ê°€ë…ì„± ì¢‹ì€ í˜•íƒœë¡œ ë³€í™˜
    """
    tables_data = []
    
    # python-docxë¡œ ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ íŒŒì‹±
    try:
        from docx import Document
        from docx.table import Table
        from docx.text.paragraph import Paragraph
    except ImportError:
        return ParsedDocument(
            text="python-docxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            blocks=[],
            metadata={"file_name": filename, "error": "python-docx not installed"},
            tables=[]
        )
    
    doc = Document(io.BytesIO(content))
    all_text = []
    current_section = None  # í˜„ì¬ ì„¹ì…˜ ì¶”ì 
    
    # ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ ìˆœíšŒ (í•µì‹¬!)
    for element in doc.element.body:
        # ë‹¨ë½(Paragraph)
        if element.tag.endswith('p'):
            para = Paragraph(element, doc)
            text = para.text.strip()
            if text:
                all_text.append(text)
                # ì„¹ì…˜ ê°ì§€ (ì˜ˆ: "3. ì±…ì„ ë° ì—­í• ")
                section_match = re.match(r'^(\d+(?:\.\d+)?)\.\s+(.+)', text)
                if section_match:
                    current_section = section_match.group(2).strip()
        
        # í‘œ(Table)
        elif element.tag.endswith('tbl'):
            table = Table(element, doc)
            rows = []
            for row in table.rows:
                cells = [cell.text.strip() for cell in row.cells]
                rows.append(cells)
            
            if rows:
                # í‘œë¥¼ ê°€ë…ì„± ì¢‹ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                table_text = _format_table_as_text(rows, current_section)
                all_text.append(table_text)
                tables_data.append({"rows": rows, "source": "python-docx"})
    
    full_text = '\n'.join(all_text)
    
    # ì¡°í•­ ë‹¨ìœ„ ë¸”ë¡ ìƒì„± (section_path í¬í•¨)
    blocks = _extract_article_blocks(full_text)
    
    # ë©”íƒ€ë°ì´í„°
    metadata = {
        "file_name": filename,
        "file_type": "docx",
        "title": _extract_title(full_text),
        "table_count": len(tables_data),
        "parser": "python-docx (ordered)"
    }
    metadata.update(_extract_sop_metadata(full_text))
    
    return ParsedDocument(
        text=full_text,
        blocks=blocks,
        metadata=metadata,
        tables=tables_data
    )


def _load_docx_basic(filename: str, content: bytes) -> ParsedDocument:
    """ê¸°ë³¸ DOCX íŒŒì‹±"""
    try:
        from docx import Document
    except ImportError:
        return ParsedDocument(
            text="DOCX íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬(python-docx)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            blocks=[],
            metadata={"file_name": filename, "error": "python-docx not installed"}
        )

    doc = Document(io.BytesIO(content))
    blocks = []
    all_text = []
    tables = []

    for para in doc.paragraphs:
        text = para.text.strip()
        if text:
            # ìŠ¤íƒ€ì¼ë¡œ íƒ€ì… ê²°ì •
            style_name = para.style.name.lower() if para.style else ""
            block_type = "title" if "heading" in style_name else "paragraph"
            blocks.append(ContentBlock(text=text, block_type=block_type))
            all_text.append(text)

    # í‘œ ì¶”ì¶œ
    for table in doc.tables:
        rows = []
        for row in table.rows:
            cells = [cell.text.strip() for cell in row.cells]
            rows.append(cells)
            all_text.append(' | '.join(cells))
        if rows:
            tables.append({"rows": rows})
            blocks.append(ContentBlock(
                text='\n'.join([' | '.join(r) for r in rows]),
                block_type="table"
            ))

    full_text = '\n\n'.join(all_text)

    if _is_article_document(full_text):
        article_blocks = _extract_article_blocks(full_text)
        if article_blocks:
            blocks = article_blocks

    metadata = {
        "file_name": filename,
        "file_type": "docx",
        "title": _extract_title(full_text),
        "table_count": len(tables),
        "parser": "python-docx"
    }
    metadata.update(_extract_sop_metadata(full_text))

    return ParsedDocument(text=full_text, blocks=blocks, metadata=metadata, tables=tables)


def _format_table_as_text(rows: List[List[str]], section_title: str = None) -> str:
    """
    í‘œë¥¼ ê°€ë…ì„± ì¢‹ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - 2ì—´ í‘œ: í‚¤-ê°’ í˜•íƒœ
    - ë‹¤ì—´ í‘œ: í—¤ë” + í–‰ í˜•íƒœ
    """
    if not rows:
        return ""
    
    # ì—´ ê°œìˆ˜ í™•ì¸
    num_cols = max(len(row) for row in rows)
    
    # 2ì—´ í‘œ: í‚¤-ê°’ í˜•íƒœ
    if num_cols == 2:
        lines = []
        if section_title:
            lines.append(f"[í‘œ: {section_title}]")
        
        # ì²« í–‰ì´ í—¤ë”ì¸ì§€ í™•ì¸ (ë‘˜ ë‹¤ ì§§ì€ í…ìŠ¤íŠ¸ë©´ í—¤ë”ë¡œ ê°„ì£¼)
        first_row = rows[0] if rows else []
        is_header = len(first_row) >= 2 and len(first_row[0]) < 10 and len(first_row[1]) < 10
        
        data_rows = rows[1:] if is_header else rows
        
        for row in data_rows:
            if len(row) >= 2:
                key = row[0].strip()
                value = row[1].strip()
                if key and value:
                    lines.append(f"â€¢ {key}: {value}")
                elif key:
                    lines.append(f"â€¢ {key}")
        
        return '\n'.join(lines)
    
    # ë‹¤ì—´ í‘œ: í—¤ë” + í–‰ í˜•íƒœ
    else:
        lines = []
        if section_title:
            lines.append(f"[í‘œ: {section_title}]")
        
        # ì²« í–‰ì„ í—¤ë”ë¡œ ê°€ì •
        if rows:
            header = rows[0]
            for row in rows[1:]:
                row_parts = []
                for i, cell in enumerate(row):
                    if cell.strip():
                        col_name = header[i] if i < len(header) else f"ì—´{i+1}"
                        row_parts.append(f"{col_name}: {cell.strip()}")
                if row_parts:
                    lines.append("â€¢ " + " | ".join(row_parts))
        
        return '\n'.join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Docling ê¸°ë°˜ íŒŒì„œ (í•µì‹¬!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _load_with_docling(filename: str, content: bytes, ext: str) -> ParsedDocument:
    """Doclingì„ ì‚¬ìš©í•œ ê³ ê¸‰ ë¬¸ì„œ íŒŒì‹±"""
    from docling.document_converter import DocumentConverter
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions

    # ì„ì‹œ íŒŒì¼ ìƒì„±
    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
        tmp.write(content)
        tmp_path = tmp.name

    try:
        # Docling ì»¨ë²„í„° ì„¤ì •
        converter = DocumentConverter()

        # ë¬¸ì„œ ë³€í™˜
        result = converter.convert(tmp_path)
        doc = result.document

        # ì „ì²´ í…ìŠ¤íŠ¸
        full_text = doc.export_to_markdown()

        # ë¸”ë¡ ì¶”ì¶œ
        blocks = []
        tables = []

        for item in doc.iterate_items():
            element = item[1] if isinstance(item, tuple) else item

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(element, 'text'):
                text = element.text
            elif hasattr(element, 'export_to_markdown'):
                text = element.export_to_markdown()
            else:
                continue

            if not text or not text.strip():
                continue

            # ë¸”ë¡ íƒ€ì… ê²°ì •
            block_type = "paragraph"
            element_type = type(element).__name__.lower()

            if "title" in element_type or "heading" in element_type:
                block_type = "title"
            elif "table" in element_type:
                block_type = "table"
                # í‘œ ë°ì´í„° ì¶”ì¶œ
                table_data = _extract_table_data(element)
                if table_data:
                    tables.append(table_data)
            elif "list" in element_type:
                block_type = "list"

            # í˜ì´ì§€ ë²ˆí˜¸
            page_num = None
            if hasattr(element, 'prov') and element.prov:
                for prov in element.prov:
                    if hasattr(prov, 'page_no'):
                        page_num = prov.page_no
                        break

            blocks.append(ContentBlock(
                text=text.strip(),
                block_type=block_type,
                page=page_num,
                metadata={"source": "docling"}
            ))

        # ì¡°í•­ íŒ¨í„´ ê°ì§€ ë° ì¬íŒŒì‹±
        if _is_article_document(full_text):
            article_blocks = _extract_article_blocks(full_text)
            if article_blocks:
                blocks = article_blocks

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = {
            "file_name": filename,
            "file_type": ext,
            "title": _extract_title(full_text),
            "total_pages": _count_pages(doc),
            "table_count": len(tables),
            "parser": "docling"
        }

        # SOP ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        sop_meta = _extract_sop_metadata(full_text)
        metadata.update(sop_meta)

        return ParsedDocument(
            text=full_text,
            blocks=blocks,
            metadata=metadata,
            tables=tables
        )

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.unlink(tmp_path)
        except Exception:
            pass


def _extract_table_data(element) -> Optional[Dict]:
    """í‘œ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    try:
        if hasattr(element, 'export_to_dataframe'):
            df = element.export_to_dataframe()
            return {
                "headers": list(df.columns),
                "rows": df.values.tolist(),
                "markdown": element.export_to_markdown() if hasattr(element, 'export_to_markdown') else str(df)
            }
        elif hasattr(element, 'data'):
            data = element.data
            if hasattr(data, 'grid'):
                return {
                    "grid": data.grid,
                    "markdown": element.export_to_markdown() if hasattr(element, 'export_to_markdown') else ""
                }
    except Exception as e:
        print(f"í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


def _count_pages(doc) -> int:
    """ë¬¸ì„œ í˜ì´ì§€ ìˆ˜"""
    try:
        if hasattr(doc, 'pages'):
            return len(doc.pages)
    except Exception:
        pass
    return 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¡°í•­ íŒŒì‹± (SOP/ë²•ë¥  ë¬¸ì„œ) - section_path ê³„ì¸µ ì¶”ì  ì¶”ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ARTICLE_PATTERNS = [
    # í•œê¸€ ì¡°í•­
    (r'^ì œ\s*(\d+)\s*ì¡°\s*(.*)', 'article'),
    (r'^ì œ\s*(\d+)\s*ì¥\s*(.*)', 'chapter'),
    (r'^ì œ\s*(\d+)\s*ì ˆ\s*(.*)', 'section'),
    
    # ğŸ”¥ "ì œ Në ˆë²¨" í˜•ì‹ (ì´ ë¬¸ì„œ ì „ìš©)
    (r'^ì œ\s*(\d+)\s*ë ˆë²¨\s*[:\(]?\s*(.+)', 'level'),  # "ì œ 1ë ˆë²¨(í’ˆì§ˆë§¤ë‰´ì–¼):"
    
    # ìˆ«ìí˜• (ì  ìˆìŒ): êµ¬ì²´ì ì¸ ê²ƒ ë¨¼ì €!
    (r'^(\d+\.\d+\.\d+)\s+([ê°€-í£A-Za-z].+)', 'subsubsection'),  # "5.1.1 Level 1"
    (r'^(\d+\.\d+)\s+([ê°€-í£A-Za-z].+)', 'subsection'),          # "6.1 ì‚¬ì „ ì¤€ë¹„"
    (r'^(\d+)\.\s+([ê°€-í£A-Za-z].+)', 'section'),                # "1. ëª©ì " (ì  ìˆìŒ)
    
    # ğŸ”¥ ìˆ«ìí˜• (ì  ì—†ìŒ): "1 ëª©ì ", "5 ì ˆì°¨" í˜•ì‹
    (r'^(\d+)\s+([ê°€-í£A-Za-z].+)', 'section'),                  # "1 ëª©ì " (ê³µë°± 1ê°œ ì´ìƒ)
    
    # ğŸ”¥ ìˆ«ì ì—†ëŠ” ì£¼ìš” ì„¹ì…˜ (ì´ ë¬¸ì„œ í˜•ì‹)
    (r'^(ëª©ì )\s*(Purpose)?', 'named_section'),
    (r'^(ì ìš©\s*ë²”ìœ„)\s*(Scope)?', 'named_section'),
    (r'^(ì •ì˜)\s*(Definitions)?', 'named_section'),
    (r'^(ì±…ì„)\s*(Responsibilities)?', 'named_section'),
    (r'^(ì ˆì°¨)\s*(Procedure)?', 'named_section'),
    (r'^(ê¸°íƒ€)\s*(.+)?', 'named_section'),
]


def _is_article_document(text: str) -> bool:
    """ì¡°í•­ ê¸°ë°˜ ë¬¸ì„œì¸ì§€ ê°ì§€"""
    patterns = [
        r'ì œ\s*\d+\s*ì¡°',
        r'ì œ\s*\d+\s*ì¥',
        r'ì œ\s*\d+\s*ì ˆ',
        r'^\d+\.\d+\.\d+',  # 5.1.1 í˜•ì‹
        r'^\d+\.\d+',       # 5.1 í˜•ì‹
        r'^SOP[-_]?\d+',
    ]

    count = 0
    for pattern in patterns:
        matches = re.findall(pattern, text, re.MULTILINE)
        count += len(matches)

    return count >= 3


def _extract_article_blocks(text: str) -> List[ContentBlock]:
    """
    ì¡°í•­ ë‹¨ìœ„ ë¸”ë¡ ì¶”ì¶œ (SOP ê²½ê³„ ê°ì§€ + section_path ê³„ì¸µ ì¶”ì )
    
    ğŸ”¥ í•µì‹¬ ê¸°ëŠ¥:
    - section_path: "5 > 5.1 > 5.1.1"
    - section_path_readable: "5 ì ˆì°¨ > 5.1 ë¬¸ì„œì²´ê³„ > 5.1.1 Level 1"
    """
    lines = text.split('\n')
    blocks = []
    current_lines = []
    current_sop_id = ""  # í˜„ì¬ SOP ID
    current_meta = {"article_num": None, "article_type": "intro", "title": ""}
    
    sop_id_pattern = re.compile(r'(SOP[-_][A-Z]+[-_]\d+)', re.IGNORECASE)

    # ğŸ”¥ ê³„ì¸µ ì¶”ì ìš© ìŠ¤íƒ
    section_stack = {
        "section": {"num": None, "title": ""},           # 5
        "subsection": {"num": None, "title": ""},        # 5.1
        "subsubsection": {"num": None, "title": ""},     # 5.1.1
    }
    
    # í•œê¸€ ì¡°í•­ìš© ìŠ¤íƒ (ì œNì¡°, ì œNì¥ ë“±)
    korean_stack = {
        "chapter": {"num": None, "title": ""},   # ì œNì¥
        "article": {"num": None, "title": ""},   # ì œNì¡°
    }

    def build_section_path() -> Dict[str, str]:
        """í˜„ì¬ ìŠ¤íƒì—ì„œ section_path ìƒì„±"""
        path_parts = []
        path_readable_parts = []
        
        # ìˆ«ì ê¸°ë°˜ (5.1.1 í˜•ì‹)
        if section_stack["section"]["num"]:
            path_parts.append(section_stack["section"]["num"])
            title = section_stack["section"]["title"]
            path_readable_parts.append(f"{section_stack['section']['num']} {title}" if title else section_stack["section"]["num"])
        
        if section_stack["subsection"]["num"]:
            path_parts.append(section_stack["subsection"]["num"])
            title = section_stack["subsection"]["title"]
            path_readable_parts.append(f"{section_stack['subsection']['num']} {title}" if title else section_stack["subsection"]["num"])
        
        if section_stack["subsubsection"]["num"]:
            path_parts.append(section_stack["subsubsection"]["num"])
            title = section_stack["subsubsection"]["title"]
            path_readable_parts.append(f"{section_stack['subsubsection']['num']} {title}" if title else section_stack["subsubsection"]["num"])
        
        # í•œê¸€ ì¡°í•­ ê¸°ë°˜ (ì œNì¥ > ì œNì¡°)
        if korean_stack["chapter"]["num"]:
            ch_num = korean_stack["chapter"]["num"]
            ch_title = korean_stack["chapter"]["title"]
            path_parts.append(f"ì œ{ch_num}ì¥")
            path_readable_parts.append(f"ì œ{ch_num}ì¥ {ch_title}" if ch_title else f"ì œ{ch_num}ì¥")
        
        if korean_stack["article"]["num"]:
            art_num = korean_stack["article"]["num"]
            art_title = korean_stack["article"]["title"]
            path_parts.append(f"ì œ{art_num}ì¡°")
            path_readable_parts.append(f"ì œ{art_num}ì¡° {art_title}" if art_title else f"ì œ{art_num}ì¡°")
        
        return {
            "section_path": " > ".join(path_parts) if path_parts else None,
            "section_path_readable": " > ".join(path_readable_parts) if path_readable_parts else None,
        }

    def flush():
        if current_lines:
            block_text = '\n'.join(current_lines).strip()
            if block_text:
                # section_path ì •ë³´ ì¶”ê°€
                path_info = build_section_path()
                
                blocks.append(ContentBlock(
                    text=block_text,
                    block_type=current_meta["article_type"],
                    section=current_meta["article_num"],
                    metadata={
                        "article_num": current_meta["article_num"],
                        "article_type": current_meta["article_type"],
                        "title": current_meta.get("title", ""),
                        "sop_id": current_sop_id,
                        "section_path": path_info["section_path"],
                        "section_path_readable": path_info["section_path_readable"],
                    }
                ))

    for line in lines:
        line_strip = line.strip()
        
        # SOP ID ì¶”ì¶œ - ìƒˆ SOP ì‹œì‘ì´ë©´ í˜„ì¬ ë¸”ë¡ flush ë¨¼ì €!
        sop_match = sop_id_pattern.search(line_strip)
        if sop_match:
            new_sop_id = sop_match.group(1).upper().replace('_', '-')
            if new_sop_id != current_sop_id:
                # ìƒˆ SOP ì‹œì‘ â†’ í˜„ì¬ ë¸”ë¡ ì €ì¥ í›„ SOP ID ê°±ì‹ 
                flush()
                current_lines = []
                current_meta = {"article_num": None, "article_type": "intro", "title": ""}
                current_sop_id = new_sop_id
                # ìŠ¤íƒ ì´ˆê¸°í™”
                section_stack = {
                    "section": {"num": None, "title": ""},
                    "subsection": {"num": None, "title": ""},
                    "subsubsection": {"num": None, "title": ""},
                }
                korean_stack = {
                    "chapter": {"num": None, "title": ""},
                    "article": {"num": None, "title": ""},
                }
        
        # ì¡°í•­ íŒ¨í„´ ë§¤ì¹­
        matched = False
        for pattern, a_type in ARTICLE_PATTERNS:
            m = re.match(pattern, line_strip)
            if m:
                flush()
                current_lines = [line]
                
                num = m.group(1)
                title = m.group(2).strip() if m.lastindex and m.lastindex >= 2 else ""
                
                # ğŸ”¥ ìŠ¤íƒ ì—…ë°ì´íŠ¸
                if a_type == "section":        # 5. ì ˆì°¨
                    section_stack["section"] = {"num": num, "title": title}
                    section_stack["subsection"] = {"num": None, "title": ""}
                    section_stack["subsubsection"] = {"num": None, "title": ""}
                
                elif a_type == "subsection":   # 5.1 ë¬¸ì„œì²´ê³„
                    section_stack["subsection"] = {"num": num, "title": title}
                    section_stack["subsubsection"] = {"num": None, "title": ""}
                
                elif a_type == "subsubsection":  # 5.1.1 Level 1
                    section_stack["subsubsection"] = {"num": num, "title": title}
                
                elif a_type == "chapter":      # ì œNì¥
                    korean_stack["chapter"] = {"num": num, "title": title}
                    korean_stack["article"] = {"num": None, "title": ""}
                
                elif a_type == "article":      # ì œNì¡°
                    korean_stack["article"] = {"num": num, "title": title}
                
                # ğŸ”¥ ìƒˆ íŒ¨í„´ ì²˜ë¦¬
                elif a_type == "level":        # ì œ Në ˆë²¨
                    section_stack["subsection"] = {"num": f"Level {num}", "title": title}
                    section_stack["subsubsection"] = {"num": None, "title": ""}
                
                elif a_type == "named_section":  # ëª©ì , ì ìš©ë²”ìœ„ ë“±
                    # ì£¼ìš” ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬ (ìŠ¤íƒ ë¦¬ì…‹)
                    section_stack["section"] = {"num": num, "title": title}
                    section_stack["subsection"] = {"num": None, "title": ""}
                    section_stack["subsubsection"] = {"num": None, "title": ""}
                
                current_meta = {
                    "article_num": num,
                    "article_type": a_type,
                    "title": title
                }
                matched = True
                break

        if not matched:
            current_lines.append(line)

    flush()
    return blocks


def _parse_articles(text: str, filename: str, ext: str) -> ParsedDocument:
    """ì¡°í•­ ë‹¨ìœ„ íŒŒì‹±"""
    blocks = _extract_article_blocks(text)
    metadata = {
        "file_name": filename,
        "file_type": ext,
        "title": _extract_title(text),
        "parser": "article"
    }
    metadata.update(_extract_sop_metadata(text))

    return ParsedDocument(text=text, blocks=blocks, metadata=metadata)


def _parse_plain_text(text: str, filename: str, ext: str) -> ParsedDocument:
    """ë‹¨ìˆœ í…ìŠ¤íŠ¸ íŒŒì‹±"""
    paragraphs = re.split(r'\n\s*\n', text)
    blocks = []

    for p in paragraphs:
        p = p.strip()
        if p:
            blocks.append(ContentBlock(text=p, block_type="paragraph"))

    return ParsedDocument(
        text=text,
        blocks=blocks,
        metadata={
            "file_name": filename,
            "file_type": ext,
            "title": _extract_title(text),
            "parser": "plain"
        }
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _extract_title(text: str) -> str:
    """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    for line in lines[:10]:
        if line.lower().startswith(("title:", "ì œëª©:")):
            return line.split(':', 1)[1].strip()
        if re.match(r'^SOP[-_]?\d+', line, re.IGNORECASE):
            return line[:100]
        if len(line) > 5 and not line.startswith('#'):
            return line[:100]
    return "ì œëª© ì—†ìŒ"


def _extract_sop_metadata(text: str) -> Dict:
    """SOP ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    metadata = {}

    # SOP ID
    sop_match = re.search(r'(SOP[-_]?[A-Z]*[-_]?\d+)', text, re.IGNORECASE)
    if sop_match:
        metadata["sop_id"] = sop_match.group(1)

    # ë²„ì „
    ver_match = re.search(r'(?:Version|Ver|ë²„ì „|ê°œì •)[\s.:]*(\d+\.?\d*)', text, re.IGNORECASE)
    if ver_match:
        metadata["version"] = ver_match.group(1)

    # ë¶€ì„œ
    dept_match = re.search(r'(?:ë¶€ì„œ|Dept|Department)[\s:]*([ê°€-í£\w\s]+?)(?:\n|$)', text, re.IGNORECASE)
    if dept_match:
        metadata["dept"] = dept_match.group(1).strip()

    # ì‹œí–‰ì¼
    date_match = re.search(r'(?:ì‹œí–‰ì¼|Effective|ë°œíš¨)[\s:]*(\d{4}[-./]\d{1,2}[-./]\d{1,2})', text, re.IGNORECASE)
    if date_match:
        metadata["effective_date"] = date_match.group(1)

    return metadata


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Fallback íŒŒì„œë“¤ (Docling ì—†ì„ ë•Œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _decode_text(content: bytes) -> str:
    """ë°”ì´íŠ¸ â†’ í…ìŠ¤íŠ¸ ë””ì½”ë”©"""
    for encoding in ["utf-8", "cp949", "euc-kr", "latin-1"]:
        try:
            return content.decode(encoding)
        except (UnicodeDecodeError, LookupError):
            continue
    return content.decode("utf-8", errors="ignore")


def _load_pdf_basic(filename: str, content: bytes) -> ParsedDocument:
    """ê¸°ë³¸ PDF íŒŒì‹± (PyMuPDF + OCR fallback)"""
    try:
        import fitz
    except ImportError:
        return ParsedDocument(
            text="PDF íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬(PyMuPDF)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            blocks=[],
            metadata={"file_name": filename, "error": "pymupdf not installed"}
        )

    doc = fitz.open(stream=content, filetype="pdf")
    blocks = []
    all_text = []
    tables = []

    for page_idx, page in enumerate(doc):
        text = page.get_text().strip()

        # í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ OCR ì‹¤í–‰
        if not text:
            ocr_text = _ocr_pdf_page(page)
            if ocr_text:
                text = ocr_text
                source = "pymupdf+ocr"
            else:
                continue
        else:
            source = "pymupdf"

        blocks.append(ContentBlock(
            text=text,
            block_type="page",
            page=page_idx + 1,
            metadata={"source": source}
        ))
        all_text.append(text)

        # í‘œ ì¶”ì¶œ ì‹œë„
        try:
            page_tables = page.find_tables()
            for table in page_tables:
                tables.append({
                    "page": page_idx + 1,
                    "rows": table.extract()
                })
        except Exception:
            pass

    full_text = "\n\n".join(all_text)

    # ì¡°í•­ ì¬íŒŒì‹±
    if _is_article_document(full_text):
        article_blocks = _extract_article_blocks(full_text)
        if article_blocks:
            blocks = article_blocks

    metadata = {
        "file_name": filename,
        "file_type": "pdf",
        "total_pages": len(doc),
        "title": _extract_title(full_text),
        "table_count": len(tables),
        "parser": "pymupdf+ocr"
    }
    metadata.update(_extract_sop_metadata(full_text))

    return ParsedDocument(
        text=full_text,
        blocks=blocks,
        metadata=metadata,
        tables=tables
    )


def _load_html_basic(filename: str, content: bytes) -> ParsedDocument:
    """ê¸°ë³¸ HTML íŒŒì‹±"""
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        text = _decode_text(content)
        return _parse_plain_text(text, filename, ".html")

    html = _decode_text(content)
    soup = BeautifulSoup(html, "html.parser")

    for tag in soup(["script", "style"]):
        tag.decompose()

    blocks = []
    tables = []

    # í‘œ ì¶”ì¶œ
    for table in soup.find_all('table'):
        rows = []
        for tr in table.find_all('tr'):
            cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
            rows.append(cells)
        if rows:
            tables.append({"rows": rows})
            blocks.append(ContentBlock(
                text='\n'.join([' | '.join(r) for r in rows]),
                block_type="table"
            ))
        table.decompose()

    # ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸
    text = soup.get_text('\n', strip=True)
    for para in text.split('\n\n'):
        para = para.strip()
        if para:
            blocks.append(ContentBlock(text=para, block_type="paragraph"))

    full_text = soup.get_text('\n', strip=True)

    return ParsedDocument(
        text=full_text,
        blocks=blocks,
        metadata={
            "file_name": filename,
            "file_type": "html",
            "title": _extract_title(full_text),
            "table_count": len(tables),
            "parser": "beautifulsoup"
        },
        tables=tables
    )


def _load_csv(filename: str, content: bytes) -> ParsedDocument:
    """CSV íŒŒì‹±"""
    text = _decode_text(content)
    lines = text.splitlines()

    rows = []
    for line in lines:
        cells = [c.strip('" ') for c in line.split(',')]
        rows.append(cells)

    table_text = '\n'.join([' | '.join(row) for row in rows])

    return ParsedDocument(
        text=table_text,
        blocks=[ContentBlock(text=table_text, block_type="table")],
        metadata={
            "file_name": filename,
            "file_type": "csv",
            "title": filename,
            "row_count": len(rows),
            "parser": "csv"
        },
        tables=[{"rows": rows}]
    )


def _ocr_pdf_page(page, ocr=None, dpi=300):
    """
    PyMuPDF page â†’ OCR í…ìŠ¤íŠ¸
    ocr: RapidOCR ê°ì²´ë¥¼ ì™¸ë¶€ì—ì„œ ì „ë‹¬ ê°€ëŠ¥ (ì¬ì‚¬ìš© ê¶Œì¥)
    """
    import tempfile
    import os
    
    try:
        from rapidocr_onnxruntime import RapidOCR
    except ImportError:
        print("âš ï¸ RapidOCR not installed, skipping OCR")
        return ""

    if ocr is None:
        ocr = RapidOCR()

    pix = page.get_pixmap(dpi=dpi)

    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp:
        pix.save(tmp.name)
        img_path = tmp.name

    try:
        result, _ = ocr(img_path)
        if result:
            texts = [text for _, text, score in result if score > 0.5]
            return "\n".join(texts)
        return ""
    except Exception as e:
        print(f"âš ï¸ OCR failed: {e}")
        return ""
    finally:
        if os.path.exists(img_path):
            os.remove(img_path)