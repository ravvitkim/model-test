"""
ChromaDB ë²¡í„° ìŠ¤í† ì–´ - ì„ë² ë”© ì €ì¥ ë° ê²€ìƒ‰
- ëª¨ë¸ë³„ ì»¬ë ‰ì…˜ ìë™ ë¶„ë¦¬
- ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ (dim â‰¤ 1024, mem â‰¤ 1300MB) â† NEW
"""

import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import hashlib
import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig


# ê¸°ë³¸ ì„¤ì •
DEFAULT_COLLECTION = "documents"
CHROMA_PATH = "./chroma_db"

# ì „ì—­ ë³€ìˆ˜
_client = None
_embed_models = {}
_device = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„ë² ë”© ëª¨ë¸ ìŠ¤í™ ì •ì˜ (í•„í„°ë§ìš©) â† NEW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EMBEDDING_MODEL_SPECS = {
    # í•œêµ­ì–´ ì „ìš© (ê¶Œì¥)
    "jhgan/ko-sroberta-multitask": {
        "name": "ko-sroberta",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ íŠ¹í™”, ê²½ëŸ‰",
    },
    "snunlp/KR-SBERT-V40K-klueNLI-augSTS": {
        "name": "ko-sbert",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ SBERT",
    },
    "BM-K/KoSimCSE-roberta": {
        "name": "ko-simcse",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ SimCSE",
    },
    
    # ë‹¤êµ­ì–´ (ê¶Œì¥)
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
        "name": "multilingual-minilm",
        "dim": 384,
        "memory_mb": 470,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ì´ˆê²½ëŸ‰",
    },
    "intfloat/multilingual-e5-large": {
        "name": "multilingual-e5",
        "dim": 1024,
        "memory_mb": 1200,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ê³ ì„±ëŠ¥",
    },
    "BAAI/bge-m3": {
        "name": "bge-m3",
        "dim": 1024,
        "memory_mb": 1300,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ê³ ì„±ëŠ¥",
    },
    
    # ì˜ì–´ ì „ìš©
    "sentence-transformers/all-MiniLM-L6-v2": {
        "name": "minilm",
        "dim": 384,
        "memory_mb": 90,
        "lang": "en",
        "desc": "ì˜ì–´, ì´ˆê²½ëŸ‰",
    },
    "sentence-transformers/all-mpnet-base-v2": {
        "name": "mpnet",
        "dim": 768,
        "memory_mb": 420,
        "lang": "en",
        "desc": "ì˜ì–´, ê³ í’ˆì§ˆ",
    },
    
    # Qwen Embedding (ì£¼ì˜: í¬ê¸° í™•ì¸ í•„ìš”)
    "Qwen/Qwen3-Embedding-0.6B": {
        "name": "qwen3-0.6b",
        "dim": 1024,
        "memory_mb": 600,
        "lang": "multi",
        "desc": "Qwen ì„ë² ë”©, ê²½ëŸ‰",
    },
    "Qwen/Qwen3-Embedding-4B": {
        "name": "qwen3-4b",
        "dim": 2560,  # âš ï¸ ì¡°ê±´ ì´ˆê³¼!
        "memory_mb": 4000,  # âš ï¸ ì¡°ê±´ ì´ˆê³¼!
        "lang": "multi",
        "desc": "âš ï¸ dim/mem ì´ˆê³¼",
        "warning": True,
    },
}

# í•„í„°ë§ ê¸°ì¤€
MAX_EMBEDDING_DIM = 1024
MAX_MEMORY_MB = 1300


def get_model_spec(model_name: str) -> Optional[Dict]:
    """ëª¨ë¸ ìŠ¤í™ ì¡°íšŒ"""
    return EMBEDDING_MODEL_SPECS.get(model_name)


def is_model_compatible(model_name: str, max_dim: int = MAX_EMBEDDING_DIM, max_mem: int = MAX_MEMORY_MB) -> Tuple[bool, str]:
    """
    ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬ (dim â‰¤ 1024, mem â‰¤ 1300MB)
    
    Returns:
        (is_compatible, message)
    """
    spec = get_model_spec(model_name)
    
    if spec is None:
        # ì•Œë ¤ì§€ì§€ ì•Šì€ ëª¨ë¸ì€ ì¼ë‹¨ í—ˆìš© (ê²½ê³ ë§Œ)
        return True, f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}. ìŠ¤í™ í™•ì¸ í•„ìš”."
    
    issues = []
    
    if spec['dim'] > max_dim:
        issues.append(f"dim={spec['dim']} > {max_dim}")
    
    if spec['memory_mb'] > max_mem:
        issues.append(f"memory={spec['memory_mb']}MB > {max_mem}MB")
    
    if issues:
        return False, f"âŒ {model_name} í˜¸í™˜ ë¶ˆê°€: {', '.join(issues)}"
    
    return True, f"âœ… {model_name} í˜¸í™˜ ê°€ëŠ¥ (dim={spec['dim']}, mem={spec['memory_mb']}MB)"


def filter_compatible_models(max_dim: int = MAX_EMBEDDING_DIM, max_mem: int = MAX_MEMORY_MB) -> List[Dict]:
    """í˜¸í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í•„í„°ë§"""
    compatible = []
    
    for model_path, spec in EMBEDDING_MODEL_SPECS.items():
        if spec['dim'] <= max_dim and spec['memory_mb'] <= max_mem:
            compatible.append({
                "path": model_path,
                **spec
            })
    
    # ë©”ëª¨ë¦¬ ìˆœ ì •ë ¬
    compatible.sort(key=lambda x: x['memory_mb'])
    return compatible


def get_embedding_model_info() -> Dict:
    """ì„ë² ë”© ëª¨ë¸ ì „ì²´ ì •ë³´ (í”„ë¡ íŠ¸ì—”ë“œìš©)"""
    all_models = []
    compatible_models = []
    incompatible_models = []
    
    for model_path, spec in EMBEDDING_MODEL_SPECS.items():
        model_info = {
            "path": model_path,
            **spec,
            "compatible": spec['dim'] <= MAX_EMBEDDING_DIM and spec['memory_mb'] <= MAX_MEMORY_MB
        }
        all_models.append(model_info)
        
        if model_info['compatible']:
            compatible_models.append(model_info)
        else:
            incompatible_models.append(model_info)
    
    return {
        "all": all_models,
        "compatible": compatible_models,
        "incompatible": incompatible_models,
        "filter_criteria": {
            "max_dim": MAX_EMBEDDING_DIM,
            "max_memory_mb": MAX_MEMORY_MB,
        }
    }


def auto_detect_model_spec(model_name: str) -> Optional[Dict]:
    """
    HuggingFaceì—ì„œ ëª¨ë¸ ìŠ¤í™ ìë™ ê°ì§€
    (ì•Œë ¤ì§€ì§€ ì•Šì€ ëª¨ë¸ìš©)
    """
    try:
        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
        
        # hidden_size ì¶”ì¶œ (ì„ë² ë”© ì°¨ì›)
        dim = getattr(config, 'hidden_size', None)
        if dim is None:
            dim = getattr(config, 'd_model', None)
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ì¶”ì • (ëŒ€ëµ 4bytes per param)
        num_params = getattr(config, 'num_parameters', None)
        if num_params is None:
            # ëª¨ë¸ ë¡œë“œí•´ì„œ í™•ì¸
            try:
                model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
                num_params = sum(p.numel() for p in model.parameters())
                del model
                torch.cuda.empty_cache()
            except:
                num_params = None
        
        memory_mb = int(num_params * 4 / 1024 / 1024) if num_params else None
        
        return {
            "name": model_name.split("/")[-1],
            "dim": dim,
            "memory_mb": memory_mb,
            "lang": "unknown",
            "desc": "ìë™ ê°ì§€",
            "auto_detected": True,
        }
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ìŠ¤í™ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_device():
    """ë””ë°”ì´ìŠ¤ í™•ì¸"""
    global _device
    if _device is None:
        _device = "cuda" if torch.cuda.is_available() else "cpu"
    return _device


def get_client() -> chromadb.PersistentClient:
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _client
    if _client is None:
        Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)
        _client = chromadb.PersistentClient(path=CHROMA_PATH)
    return _client


def get_embedding_model(model_name: str = "jhgan/ko-sroberta-multitask", check_compatibility: bool = True):
    """
    ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ëª¨ë¸ë³„ ìºì‹±)
    
    Args:
        model_name: ëª¨ë¸ ê²½ë¡œ
        check_compatibility: í˜¸í™˜ì„± ê²€ì‚¬ ì—¬ë¶€
    """
    global _embed_models
    
    # í˜¸í™˜ì„± ê²€ì‚¬
    if check_compatibility:
        is_ok, msg = is_model_compatible(model_name)
        print(msg)
        if not is_ok:
            raise ValueError(f"ëª¨ë¸ í˜¸í™˜ì„± ì˜¤ë¥˜: {msg}")
    
    if model_name in _embed_models:
        return _embed_models[model_name]
    
    print(f"ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(get_device())
    model.eval()
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} (device: {get_device()})")
    
    _embed_models[model_name] = (tokenizer, model)
    return tokenizer, model


def get_collection_name_for_model(base_name: str, model_name: str) -> str:
    """ëª¨ë¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±"""
    safe_model = model_name.split("/")[-1].replace("-", "_").replace(".", "_")
    return f"{base_name}__{safe_model}"


def embed_text(text: str, model_name: str = "jhgan/ko-sroberta-multitask") -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    tokenizer, model = get_embedding_model(model_name)
    
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(get_device())
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Mean Pooling
    attention_mask = inputs['attention_mask']
    token_embeddings = outputs.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    embedding = (sum_embeddings / sum_mask).cpu().numpy()
    
    return embedding[0].tolist()


def embed_texts(texts: List[str], model_name: str = "jhgan/ko-sroberta-multitask") -> List[List[float]]:
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©"""
    return [embed_text(text, model_name) for text in texts]


def generate_doc_id(text: str, doc_name: str = "") -> str:
    """ë¬¸ì„œ ID ìƒì„±"""
    content = f"{doc_name}:{text[:100]}"
    return hashlib.md5(content.encode()).hexdigest()[:16]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì»¬ë ‰ì…˜ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_collection(name: str, model_name: str = None) -> chromadb.Collection:
    """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
    client = get_client()
    
    if model_name:
        name = get_collection_name_for_model(name, model_name)
    
    return client.get_or_create_collection(
        name=name,
        metadata={"hnsw:space": "cosine"}
    )


def delete_collection(name: str) -> bool:
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    try:
        client = get_client()
        client.delete_collection(name=name)
        return True
    except Exception:
        return False


def list_collections() -> List[str]:
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    client = get_client()
    return [col.name for col in client.list_collections()]


def get_collection_info(name: str) -> Dict:
    """ì»¬ë ‰ì…˜ ì •ë³´"""
    try:
        client = get_client()
        collection = client.get_collection(name=name)
        
        model_info = "unknown"
        if "__" in name:
            model_info = name.split("__")[-1]
        
        return {
            "name": name,
            "count": collection.count(),
            "model": model_info,
            "metadata": collection.metadata
        }
    except Exception as e:
        return {"name": name, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ì¶”ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def add_documents(
    chunks: List[str],
    doc_name: str,
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "jhgan/ko-sroberta-multitask",
    metadata_list: Optional[List[Dict]] = None
) -> Dict:
    """ë¬¸ì„œ ì²­í¬ë“¤ì„ ChromaDBì— ì €ì¥ (ëª¨ë¸ë³„ ì»¬ë ‰ì…˜)"""
    
    actual_collection_name = get_collection_name_for_model(collection_name, model_name)
    collection = create_collection(collection_name, model_name)
    
    ids = []
    embeddings = []
    metadatas = []
    documents = []
    
    for i, chunk in enumerate(chunks):
        if not chunk.strip():
            continue
            
        doc_id = generate_doc_id(chunk, f"{doc_name}_{i}")
        embedding = embed_text(chunk, model_name)
        
        # ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        if metadata_list and i < len(metadata_list):
            meta = metadata_list[i].copy()
        else:
            meta = {
                "doc_name": doc_name,
                "chunk_index": i,
                "total_chunks": len(chunks)
            }
        
        # í•„ìˆ˜ í•„ë“œ ë³´ì¥
        meta["doc_name"] = meta.get("doc_name", doc_name)
        meta["model"] = model_name
        
        # None ê°’ ì œê±° (ChromaDB í˜¸í™˜)
        meta = {k: v for k, v in meta.items() if v is not None}
        
        ids.append(doc_id)
        embeddings.append(embedding)
        documents.append(chunk)
        metadatas.append(meta)
    
    if ids:
        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
    
    return {
        "success": True,
        "doc_name": doc_name,
        "chunks_added": len(ids),
        "collection": actual_collection_name,
        "model": model_name
    }


def add_single_text(
    text: str,
    doc_name: str = "manual_input",
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "jhgan/ko-sroberta-multitask"
) -> Dict:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¶”ê°€"""
    return add_documents([text], doc_name, collection_name, model_name)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def search(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 5,
    model_name: str = "jhgan/ko-sroberta-multitask",
    filter_doc: str = None
) -> List[Dict]:
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ëª¨ë¸ë³„ ì»¬ë ‰ì…˜)"""
    
    actual_collection_name = get_collection_name_for_model(collection_name, model_name)
    
    try:
        client = get_client()
        collection = client.get_collection(name=actual_collection_name)
    except Exception:
        return []
    
    if collection.count() == 0:
        return []
    
    query_embedding = embed_text(query, model_name)
    
    # í•„í„° ì„¤ì •
    where_filter = None
    if filter_doc:
        where_filter = {"doc_name": filter_doc}
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=min(n_results, collection.count()),
        where=where_filter,
        include=["documents", "metadatas", "distances"]
    )
    
    search_results = []
    
    if results['documents'] and results['documents'][0]:
        for i, doc in enumerate(results['documents'][0]):
            distance = results['distances'][0][i] if results['distances'] else 0
            similarity = 1 - distance
            
            search_results.append({
                "text": doc,
                "similarity": round(max(0, min(1, similarity)), 4),
                "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                "id": results['ids'][0][i] if results['ids'] else None
            })
    
    return search_results


def search_with_context(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 3,
    model_name: str = "jhgan/ko-sroberta-multitask",
    filter_doc: str = None
) -> Tuple[List[Dict], str]:
    """ê²€ìƒ‰ + ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    results = search(query, collection_name, n_results, model_name, filter_doc)
    
    context_parts = []
    for i, r in enumerate(results):
        meta = r.get('metadata', {})
        
        # ì¡°í•­ ì •ë³´ í¬í•¨
        header = f"[ë¬¸ì„œ {i+1}]"
        if meta.get('doc_name'):
            header = f"[{meta['doc_name']}"
            if meta.get('article_num'):
                article_type = meta.get('article_type', 'article')
                if article_type == 'article':
                    header += f" - ì œ{meta['article_num']}ì¡°"
                elif article_type == 'chapter':
                    header += f" - ì œ{meta['article_num']}ì¥"
                else:
                    header += f" - {meta['article_num']}"
            header += f"] (ìœ ì‚¬ë„: {r['similarity']:.1%})"
        
        context_parts.append(f"{header}\n{r['text']}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    return results, context


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ì‚­ì œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def delete_by_doc_name(
    doc_name: str, 
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = None
) -> Dict:
    """ë¬¸ì„œ ì´ë¦„ìœ¼ë¡œ ì‚­ì œ"""
    
    if model_name:
        actual_name = get_collection_name_for_model(collection_name, model_name)
        try:
            client = get_client()
            collection = client.get_collection(name=actual_name)
            results = collection.get(where={"doc_name": doc_name}, include=["metadatas"])
            if results['ids']:
                collection.delete(ids=results['ids'])
                return {"success": True, "deleted": len(results['ids']), "collection": actual_name}
        except:
            pass
        return {"success": False, "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}
    
    deleted_total = 0
    for col_name in list_collections():
        if col_name.startswith(collection_name + "__"):
            try:
                client = get_client()
                collection = client.get_collection(name=col_name)
                results = collection.get(where={"doc_name": doc_name}, include=["metadatas"])
                if results['ids']:
                    collection.delete(ids=results['ids'])
                    deleted_total += len(results['ids'])
            except:
                continue
    
    if deleted_total > 0:
        return {"success": True, "deleted": deleted_total}
    return {"success": False, "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


def delete_all(collection_name: str = DEFAULT_COLLECTION, model_name: str = None) -> Dict:
    """ì»¬ë ‰ì…˜ ë‚´ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ"""
    try:
        if model_name:
            actual_name = get_collection_name_for_model(collection_name, model_name)
            delete_collection(actual_name)
            return {"success": True, "message": f"{actual_name} ì‚­ì œë¨"}
        
        deleted = []
        for col_name in list_collections():
            if col_name.startswith(collection_name + "__") or col_name == collection_name:
                delete_collection(col_name)
                deleted.append(col_name)
        
        return {"success": True, "deleted_collections": deleted}
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ëª©ë¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def list_documents(collection_name: str = DEFAULT_COLLECTION, model_name: str = None) -> List[Dict]:
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡"""
    
    docs = {}
    
    if model_name:
        target_collections = [get_collection_name_for_model(collection_name, model_name)]
    else:
        target_collections = [
            col for col in list_collections() 
            if col.startswith(collection_name + "__") or col == collection_name
        ]
    
    for col_name in target_collections:
        try:
            client = get_client()
            collection = client.get_collection(name=col_name)
            results = collection.get(include=["metadatas"])
            
            for meta in (results['metadatas'] or []):
                doc_name = meta.get('doc_name', 'unknown')
                model = meta.get('model', 'unknown')
                key = f"{doc_name}|{model}"
                
                if key not in docs:
                    docs[key] = {
                        "doc_name": doc_name,
                        "doc_title": meta.get('doc_title'),
                        "model": model,
                        "collection": col_name,
                        "chunk_count": 0,
                        "chunk_method": meta.get('chunk_method'),
                    }
                docs[key]["chunk_count"] += 1
        except:
            continue
    
    return list(docs.values())