"""
ChromaDB ë²¡í„° ìŠ¤í† ì–´ - ë¦¬íŒ©í† ë§ v5.1
- ìœ ì‚¬ë„ threshold ì¶”ê°€ (ë‚®ì€ í’ˆì§ˆ ê²°ê³¼ í•„í„°ë§)
- ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ì¶”ê°€
- ì½”ë“œ êµ¬ì¡° ê°œì„ 
"""

import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import hashlib
import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig
from dataclasses import dataclass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì • ìƒìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_COLLECTION = "documents"
CHROMA_PATH = "./chroma_db"

# ê²€ìƒ‰ í’ˆì§ˆ ì„¤ì •
DEFAULT_SIMILARITY_THRESHOLD = 0.35  # ì´ ì´í•˜ëŠ” "ê´€ë ¨ ì—†ìŒ"ìœ¼ë¡œ íŒë‹¨
HIGH_CONFIDENCE_THRESHOLD = 0.65     # ì´ ì´ìƒì€ "ì‹ ë¢°ë„ ë†’ìŒ"
MIN_RESULTS_BEFORE_FILTER = 3        # í•„í„°ë§ ì „ ìµœì†Œ ê²°ê³¼ ìˆ˜

# ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ ê¸°ì¤€
MAX_EMBEDDING_DIM = 1024
MAX_MEMORY_MB = 1300

# ì „ì—­ ìºì‹œ
_client: Optional[chromadb.PersistentClient] = None
_embed_models: Dict = {}
_device: Optional[str] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„ë² ë”© ëª¨ë¸ ìŠ¤í™ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EMBEDDING_MODEL_SPECS = {
    # í•œêµ­ì–´ ì „ìš© (ê¶Œì¥)
    "jhgan/ko-sroberta-multitask": {
        "name": "ko-sroberta",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ íŠ¹í™”, ê²½ëŸ‰",
    },
    "snunlp/KR-SBERT-V40K-klueNLI-augSTS": {
        "name": "ko-sbert",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ SBERT",
    },
    "BM-K/KoSimCSE-roberta": {
        "name": "ko-simcse",
        "dim": 768,
        "memory_mb": 440,
        "lang": "ko",
        "desc": "í•œêµ­ì–´ SimCSE",
    },
    
    # ë‹¤êµ­ì–´ (ê¶Œì¥)
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
        "name": "multilingual-minilm",
        "dim": 384,
        "memory_mb": 470,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ì´ˆê²½ëŸ‰",
    },
    "intfloat/multilingual-e5-large": {
        "name": "multilingual-e5",
        "dim": 1024,
        "memory_mb": 1200,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ê³ ì„±ëŠ¥",
    },
    "BAAI/bge-m3": {
        "name": "bge-m3",
        "dim": 1024,
        "memory_mb": 1300,
        "lang": "multi",
        "desc": "ë‹¤êµ­ì–´, ê³ ì„±ëŠ¥",
    },
    
    # ì˜ì–´ ì „ìš©
    "sentence-transformers/all-MiniLM-L6-v2": {
        "name": "minilm",
        "dim": 384,
        "memory_mb": 90,
        "lang": "en",
        "desc": "ì˜ì–´, ì´ˆê²½ëŸ‰",
    },
    "sentence-transformers/all-mpnet-base-v2": {
        "name": "mpnet",
        "dim": 768,
        "memory_mb": 420,
        "lang": "en",
        "desc": "ì˜ì–´, ê³ í’ˆì§ˆ",
    },
    
    # Qwen Embedding
    "Qwen/Qwen3-Embedding-0.6B": {
        "name": "qwen3-0.6b",
        "dim": 1024,
        "memory_mb": 600,
        "lang": "multi",
        "desc": "Qwen ì„ë² ë”©, ê²½ëŸ‰",
    },
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë‹¨ì¼ í•­ëª©"""
    text: str
    similarity: float
    metadata: Dict
    id: str
    confidence: str  # "high", "medium", "low"
    
    def to_dict(self) -> Dict:
        return {
            "text": self.text,
            "similarity": self.similarity,
            "metadata": self.metadata,
            "id": self.id,
            "confidence": self.confidence,
        }


@dataclass  
class SearchResponse:
    """ê²€ìƒ‰ ì‘ë‹µ ì „ì²´"""
    results: List[SearchResult]
    query: str
    total_found: int
    filtered_count: int
    quality_summary: Dict
    
    def to_dict(self) -> Dict:
        return {
            "results": [r.to_dict() for r in self.results],
            "query": self.query,
            "total_found": self.total_found,
            "filtered_count": self.filtered_count,
            "quality_summary": self.quality_summary,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_model_spec(model_name: str) -> Optional[Dict]:
    """ëª¨ë¸ ìŠ¤í™ ì¡°íšŒ"""
    return EMBEDDING_MODEL_SPECS.get(model_name)


def is_model_compatible(
    model_name: str, 
    max_dim: int = MAX_EMBEDDING_DIM, 
    max_mem: int = MAX_MEMORY_MB
) -> Tuple[bool, str]:
    """ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬"""
    spec = get_model_spec(model_name)
    
    if spec is None:
        return True, f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}. ìŠ¤í™ í™•ì¸ í•„ìš”."
    
    issues = []
    if spec['dim'] > max_dim:
        issues.append(f"dim={spec['dim']} > {max_dim}")
    if spec['memory_mb'] > max_mem:
        issues.append(f"memory={spec['memory_mb']}MB > {max_mem}MB")
    
    if issues:
        return False, f"âŒ {model_name} í˜¸í™˜ ë¶ˆê°€: {', '.join(issues)}"
    
    return True, f"âœ… {model_name} í˜¸í™˜ ê°€ëŠ¥ (dim={spec['dim']}, mem={spec['memory_mb']}MB)"


def filter_compatible_models(
    max_dim: int = MAX_EMBEDDING_DIM, 
    max_mem: int = MAX_MEMORY_MB
) -> List[Dict]:
    """í˜¸í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    compatible = []
    for model_path, spec in EMBEDDING_MODEL_SPECS.items():
        if spec['dim'] <= max_dim and spec['memory_mb'] <= max_mem:
            compatible.append({"path": model_path, **spec})
    compatible.sort(key=lambda x: x['memory_mb'])
    return compatible


def get_embedding_model_info() -> Dict:
    """ì„ë² ë”© ëª¨ë¸ ì „ì²´ ì •ë³´"""
    all_models = []
    compatible = []
    incompatible = []
    
    for model_path, spec in EMBEDDING_MODEL_SPECS.items():
        is_compat = spec['dim'] <= MAX_EMBEDDING_DIM and spec['memory_mb'] <= MAX_MEMORY_MB
        model_info = {"path": model_path, **spec, "compatible": is_compat}
        all_models.append(model_info)
        (compatible if is_compat else incompatible).append(model_info)
    
    return {
        "all": all_models,
        "compatible": compatible,
        "incompatible": incompatible,
        "filter_criteria": {"max_dim": MAX_EMBEDDING_DIM, "max_memory_mb": MAX_MEMORY_MB}
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_device() -> str:
    """ë””ë°”ì´ìŠ¤ í™•ì¸"""
    global _device
    if _device is None:
        _device = "cuda" if torch.cuda.is_available() else "cpu"
    return _device


def get_client() -> chromadb.PersistentClient:
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _client
    if _client is None:
        Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)
        _client = chromadb.PersistentClient(path=CHROMA_PATH)
    return _client


def get_embedding_model(
    model_name: str = "jhgan/ko-sroberta-multitask",
    check_compatibility: bool = True
):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    global _embed_models
    
    if model_name in _embed_models:
        return _embed_models[model_name]
    
    # í˜¸í™˜ì„± ê²€ì‚¬
    if check_compatibility:
        is_ok, msg = is_model_compatible(model_name)
        if not is_ok:
            raise ValueError(msg)
        print(msg)
    
    print(f"ğŸ“¦ Loading embedding model: {model_name}...")
    device = get_device()
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)
    model.eval()
    
    _embed_models[model_name] = (tokenizer, model)
    print(f"âœ… Embedding model loaded: {model_name}")
    return tokenizer, model


def embed_text(text: str, model_name: str = "jhgan/ko-sroberta-multitask") -> List[float]:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    tokenizer, model = get_embedding_model(model_name)
    device = get_device()
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í°í™” ì „)
    MAX_CHARS = 1500
    if len(text) > MAX_CHARS:
        text = text[:MAX_CHARS]
    
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Mean pooling
    attention_mask = inputs['attention_mask']
    token_embeddings = outputs.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    embedding = (sum_embeddings / sum_mask).cpu().numpy()
    
    return embedding[0].tolist()


def generate_doc_id(text: str, prefix: str = "") -> str:
    """ë¬¸ì„œ ID ìƒì„±"""
    hash_val = hashlib.md5(text.encode()).hexdigest()[:12]
    return f"{prefix}_{hash_val}" if prefix else hash_val


def get_collection_name_for_model(base_name: str, model_name: str) -> str:
    """ëª¨ë¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±"""
    model_suffix = model_name.replace("/", "_").replace("-", "_")
    return f"{base_name}__{model_suffix}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì»¬ë ‰ì…˜ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_collection(
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "jhgan/ko-sroberta-multitask"
):
    """ì»¬ë ‰ì…˜ ìƒì„±/ê°€ì ¸ì˜¤ê¸°"""
    client = get_client()
    actual_name = get_collection_name_for_model(collection_name, model_name)
    return client.get_or_create_collection(
        name=actual_name,
        metadata={"hnsw:space": "cosine", "embedding_model": model_name}
    )


def list_collections() -> List[str]:
    """ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡"""
    client = get_client()
    return [c.name for c in client.list_collections()]


def get_collection_info(collection_name: str) -> Dict:
    """ì»¬ë ‰ì…˜ ì •ë³´"""
    try:
        client = get_client()
        collection = client.get_collection(name=collection_name)
        return {
            "name": collection_name,
            "count": collection.count(),
            "metadata": collection.metadata
        }
    except Exception:
        return {"name": collection_name, "count": 0, "error": "not found"}


def delete_collection(collection_name: str) -> bool:
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    try:
        client = get_client()
        client.delete_collection(name=collection_name)
        return True
    except Exception:
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def add_documents(
    chunks: List[str],
    doc_name: str,
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "jhgan/ko-sroberta-multitask",
    metadata_list: Optional[List[Dict]] = None
) -> Dict:
    """ë¬¸ì„œ ì²­í¬ë“¤ì„ ChromaDBì— ì €ì¥"""
    
    collection = create_collection(collection_name, model_name)
    actual_collection_name = get_collection_name_for_model(collection_name, model_name)
    
    ids, embeddings, metadatas, documents = [], [], [], []
    
    for i, chunk in enumerate(chunks):
        if not chunk.strip():
            continue
        
        doc_id = generate_doc_id(chunk, f"{doc_name}_{i}")
        embedding = embed_text(chunk, model_name)
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        meta = metadata_list[i].copy() if metadata_list and i < len(metadata_list) else {}
        
        # SOP ë¬¸ì„œ í˜•ì‹ì´ë©´ ì§€ì •ëœ í•„ë“œ ì™¸ì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        if meta.get("doc_type") == "SOP":
            pass # ì´ë¯¸ chunkerì—ì„œ 11ê°œ í•„ë“œë¥¼ ë§ì¶°ì¤Œ
        else:
            meta.update({
                "doc_name": doc_name,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "model": model_name,
                "char_count": len(chunk),
            })
            
        # None ê°’ ì œê±° (ChromaDB í˜¸í™˜)
        meta = {k: v for k, v in meta.items() if v is not None}
        
        ids.append(doc_id)
        embeddings.append(embedding)
        documents.append(chunk)
        metadatas.append(meta)
    
    if ids:
        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
    
    return {
        "success": True,
        "doc_name": doc_name,
        "chunks_added": len(ids),
        "collection": actual_collection_name,
        "model": model_name
    }


def add_single_text(
    text: str,
    doc_name: str = "manual_input",
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "jhgan/ko-sroberta-multitask"
) -> Dict:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¶”ê°€"""
    return add_documents([text], doc_name, collection_name, model_name)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²€ìƒ‰ (í•µì‹¬ ê°œì„ !)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _classify_confidence(similarity: float) -> str:
    """ìœ ì‚¬ë„ ê¸°ë°˜ ì‹ ë¢°ë„ ë¶„ë¥˜"""
    if similarity >= HIGH_CONFIDENCE_THRESHOLD:
        return "high"
    elif similarity >= DEFAULT_SIMILARITY_THRESHOLD:
        return "medium"
    return "low"


def _calculate_quality_summary(results: List[SearchResult]) -> Dict:
    """ê²€ìƒ‰ í’ˆì§ˆ ìš”ì•½ ê³„ì‚°"""
    if not results:
        return {"avg_similarity": 0, "high_count": 0, "medium_count": 0, "low_count": 0}
    
    similarities = [r.similarity for r in results]
    return {
        "avg_similarity": round(sum(similarities) / len(similarities), 4),
        "max_similarity": round(max(similarities), 4),
        "min_similarity": round(min(similarities), 4),
        "high_count": sum(1 for r in results if r.confidence == "high"),
        "medium_count": sum(1 for r in results if r.confidence == "medium"),
        "low_count": sum(1 for r in results if r.confidence == "low"),
    }


def search(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 5,
    model_name: str = "jhgan/ko-sroberta-multitask",
    filter_doc: Optional[str] = None,
    similarity_threshold: Optional[float] = None,
    return_low_confidence: bool = True,
) -> List[Dict]:
    """
    ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        model_name: ì„ë² ë”© ëª¨ë¸
        filter_doc: íŠ¹ì • ë¬¸ì„œë§Œ ê²€ìƒ‰
        similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        return_low_confidence: Falseë©´ ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ ì œì™¸
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (dict í˜•íƒœ)
    """
    actual_collection_name = get_collection_name_for_model(collection_name, model_name)
    
    try:
        client = get_client()
        collection = client.get_collection(name=actual_collection_name)
    except Exception:
        return []
    
    if collection.count() == 0:
        return []
    
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = embed_text(query, model_name)
    
    # í•„í„° ì„¤ì •
    where_filter = {"doc_name": filter_doc} if filter_doc else None
    
    # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§ (í’ˆì§ˆ í–¥ìƒ)
    fetch_count = min(n_results * 2, collection.count())
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=fetch_count,
        where=where_filter,
        include=["documents", "metadatas", "distances"]
    )
    
    # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
    threshold = similarity_threshold if similarity_threshold is not None else DEFAULT_SIMILARITY_THRESHOLD
    search_results = []
    
    if results['documents'] and results['documents'][0]:
        for i, doc in enumerate(results['documents'][0]):
            distance = results['distances'][0][i] if results['distances'] else 0
            similarity = max(0, min(1, 1 - distance))  # [0, 1] ë²”ìœ„ë¡œ í´ë¨í•‘
            
            confidence = _classify_confidence(similarity)
            
            # ë‚®ì€ ì‹ ë¢°ë„ í•„í„°ë§
            if not return_low_confidence and confidence == "low":
                continue
            
            # threshold ë¯¸ë§Œ í•„í„°ë§ (ë‹¨, ìµœì†Œ ê²°ê³¼ëŠ” ë³´ì¥)
            if similarity < threshold and len(search_results) >= MIN_RESULTS_BEFORE_FILTER:
                continue
            
            search_results.append({
                "text": doc,
                "similarity": round(similarity, 4),
                "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                "id": results['ids'][0][i] if results['ids'] else None,
                "confidence": confidence,
            })
    
    # ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
    return search_results[:n_results]


def search_with_context(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 3,
    model_name: str = "jhgan/ko-sroberta-multitask",
    filter_doc: Optional[str] = None,
    similarity_threshold: Optional[float] = None,
) -> Tuple[List[Dict], str]:
    """ê²€ìƒ‰ + ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±"""
    
    results = search(
        query=query,
        collection_name=collection_name,
        n_results=n_results,
        model_name=model_name,
        filter_doc=filter_doc,
        similarity_threshold=similarity_threshold,
        return_low_confidence=True,  # ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì‹œì—ëŠ” ì¼ë‹¨ í¬í•¨
    )
    
    context_parts = []
    for i, r in enumerate(results):
        meta = r.get('metadata', {})
        confidence = r.get('confidence', 'medium')
        
        # í—¤ë” êµ¬ì„±
        header_parts = []
        
        # ë¬¸ì„œëª…
        doc_name = meta.get('doc_name', f'ë¬¸ì„œ {i+1}')
        header_parts.append(doc_name)
        
        # ì¡°í•­ ì •ë³´
        article_num = meta.get('article_num')
        article_type = meta.get('article_type', 'article')
        if article_num:
            if article_type == 'article':
                header_parts.append(f"ì œ{article_num}ì¡°")
            elif article_type == 'chapter':
                header_parts.append(f"ì œ{article_num}ì¥")
            else:
                header_parts.append(str(article_num))
        
        # ìœ ì‚¬ë„ ë° ì‹ ë¢°ë„
        sim_str = f"{r['similarity']:.1%}"
        conf_emoji = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}.get(confidence, "âšª")
        
        header = f"[{' - '.join(header_parts)}] ({sim_str} {conf_emoji})"
        context_parts.append(f"{header}\n{r['text']}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    return results, context


def search_advanced(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 5,
    model_name: str = "jhgan/ko-sroberta-multitask",
    filter_doc: Optional[str] = None,
    similarity_threshold: Optional[float] = None,
) -> SearchResponse:
    """ê³ ê¸‰ ê²€ìƒ‰ (í’ˆì§ˆ ë©”íŠ¸ë¦­ í¬í•¨)"""
    
    # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë¶„ì„
    all_results = search(
        query=query,
        collection_name=collection_name,
        n_results=n_results * 2,
        model_name=model_name,
        filter_doc=filter_doc,
        similarity_threshold=0.0,  # ì¼ë‹¨ ë‹¤ ê°€ì ¸ì˜´
        return_low_confidence=True,
    )
    
    # SearchResult ê°ì²´ë¡œ ë³€í™˜
    result_objects = [
        SearchResult(
            text=r['text'],
            similarity=r['similarity'],
            metadata=r['metadata'],
            id=r['id'],
            confidence=r['confidence']
        )
        for r in all_results
    ]
    
    # threshold ì ìš© í•„í„°ë§
    threshold = similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    filtered = [r for r in result_objects if r.similarity >= threshold]
    
    # ìµœì†Œ ê²°ê³¼ ë³´ì¥
    if len(filtered) < MIN_RESULTS_BEFORE_FILTER:
        filtered = result_objects[:MIN_RESULTS_BEFORE_FILTER]
    
    # ìš”ì²­ ê°œìˆ˜ë¡œ ì œí•œ
    final_results = filtered[:n_results]
    
    return SearchResponse(
        results=final_results,
        query=query,
        total_found=len(all_results),
        filtered_count=len(filtered),
        quality_summary=_calculate_quality_summary(final_results)
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ì‚­ì œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def delete_by_doc_name(
    doc_name: str,
    collection_name: str = DEFAULT_COLLECTION,
    model_name: Optional[str] = None
) -> Dict:
    """ë¬¸ì„œ ì´ë¦„ìœ¼ë¡œ ì‚­ì œ"""
    
    if model_name:
        actual_name = get_collection_name_for_model(collection_name, model_name)
        try:
            client = get_client()
            collection = client.get_collection(name=actual_name)
            results = collection.get(where={"doc_name": doc_name}, include=["metadatas"])
            if results['ids']:
                collection.delete(ids=results['ids'])
                return {"success": True, "deleted": len(results['ids']), "collection": actual_name}
        except Exception:
            pass
        return {"success": False, "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}
    
    # ëª¨ë“  ê´€ë ¨ ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
    deleted_total = 0
    for col_name in list_collections():
        if col_name.startswith(collection_name + "__"):
            try:
                client = get_client()
                collection = client.get_collection(name=col_name)
                results = collection.get(where={"doc_name": doc_name}, include=["metadatas"])
                if results['ids']:
                    collection.delete(ids=results['ids'])
                    deleted_total += len(results['ids'])
            except Exception:
                continue
    
    if deleted_total > 0:
        return {"success": True, "deleted": deleted_total}
    return {"success": False, "message": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


def delete_all(
    collection_name: str = DEFAULT_COLLECTION,
    model_name: Optional[str] = None
) -> Dict:
    """ì»¬ë ‰ì…˜ ë‚´ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ"""
    try:
        if model_name:
            actual_name = get_collection_name_for_model(collection_name, model_name)
            delete_collection(actual_name)
            return {"success": True, "message": f"{actual_name} ì‚­ì œë¨"}
        
        deleted = []
        for col_name in list_collections():
            if col_name.startswith(collection_name + "__") or col_name == collection_name:
                delete_collection(col_name)
                deleted.append(col_name)
        
        return {"success": True, "deleted_collections": deleted}
    except Exception as e:
        return {"success": False, "error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¬¸ì„œ ëª©ë¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def list_documents(
    collection_name: str = DEFAULT_COLLECTION,
    model_name: Optional[str] = None
) -> List[Dict]:
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡"""
    
    docs = {}
    
    if model_name:
        target_collections = [get_collection_name_for_model(collection_name, model_name)]
    else:
        target_collections = [
            col for col in list_collections()
            if col.startswith(collection_name + "__") or col == collection_name
        ]
    
    for col_name in target_collections:
        try:
            client = get_client()
            collection = client.get_collection(name=col_name)
            results = collection.get(include=["metadatas"])
            
            for meta in (results['metadatas'] or []):
                doc_name = meta.get('doc_name', 'unknown')
                model = meta.get('model', 'unknown')
                key = f"{doc_name}|{model}"
                
                if key not in docs:
                    docs[key] = {
                        "doc_name": doc_name,
                        "doc_title": meta.get('doc_title'),
                        "model": model,
                        "collection": col_name,
                        "chunk_count": 0,
                        "chunk_method": meta.get('chunk_method'),
                    }
                docs[key]["chunk_count"] += 1
        except Exception:
            continue
    
    return list(docs.values())
