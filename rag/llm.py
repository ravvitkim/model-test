"""
LLM λ¨λ“ v6.0 - Ollama + HuggingFace
"""

import torch
import requests
from typing import Dict, List, Optional, Any


device = "cuda" if torch.cuda.is_available() else "cpu"
_loaded_llm: Dict[str, Any] = {}


# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•
# Ollama λ°±μ—”λ“
# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•

class OllamaLLM:
    """Ollama λ„¤μ΄ν‹°λΈ API"""

    def __init__(self, model: str = "qwen2.5:3b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url

    def generate(
        self,
        prompt: str,
        system: str = None,
        temperature: float = 0.1,
        max_tokens: int = 256
    ) -> str:
        """ν…μ¤νΈ μƒμ„±"""
        try:
            return self._call_chat_api(prompt, system, temperature, max_tokens)
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return self._call_generate_api(prompt, system, temperature, max_tokens)
            raise
        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"Ollama μ„λ²„ μ—°κ²° μ‹¤ν¨. 'ollama serve' μ‹¤ν–‰ ν•„μ”")

    def _call_chat_api(
        self,
        prompt: str,
        system: str,
        temperature: float,
        max_tokens: int
    ) -> str:
        """/api/chat μ—”λ“ν¬μΈνΈ"""
        messages = []
        if system:
            messages.append({"role": "system", "content": system})

        # Qwen3 thinking λ¨λ“ λΉ„ν™μ„±ν™”
        final_prompt = f"/no_think {prompt}" if "qwen3" in self.model.lower() else prompt
        messages.append({"role": "user", "content": final_prompt})

        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {"temperature": temperature, "num_predict": max_tokens},
                "think": False,
            },
            timeout=120
        )
        response.raise_for_status()
        data = response.json()

        message = data.get("message", {})
        content = message.get("content", "") if isinstance(message, dict) else str(message)

        # thinking fallback
        if not content and isinstance(message, dict) and message.get("thinking"):
            content = message.get("thinking", "")

        return content

    def _call_generate_api(
        self,
        prompt: str,
        system: str,
        temperature: float,
        max_tokens: int
    ) -> str:
        """/api/generate μ—”λ“ν¬μΈνΈ (κµ¬λ²„μ „)"""
        full_prompt = f"{system}\n\n{prompt}" if system else prompt

        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": full_prompt,
                "stream": False,
                "options": {"temperature": temperature, "num_predict": max_tokens},
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("response", "")

    @staticmethod
    def list_models(base_url: str = "http://localhost:11434") -> List[str]:
        """μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅"""
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.ok:
                return [m["name"] for m in response.json().get("models", [])]
        except Exception:
            pass
        return []

    @staticmethod
    def is_available(base_url: str = "http://localhost:11434") -> bool:
        """μ„λ²„ μ‹¤ν–‰ μ—¬λ¶€"""
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=3)
            return response.ok
        except Exception:
            return False


# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•
# HuggingFace λ°±μ—”λ“
# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•

def load_llm(model_name: str):
    """HuggingFace LLM λ΅λ“"""
    if model_name in _loaded_llm:
        return _loaded_llm[model_name]

    from transformers import AutoTokenizer, AutoModelForCausalLM

    print(f"π¤– Loading LLM: {model_name}...")

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    dtype = torch.float16 if device == "cuda" else torch.float32

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map="auto" if device == "cuda" else None
    )

    if device == "cpu":
        model = model.to(device)
    model.eval()

    _loaded_llm[model_name] = (tokenizer, model)
    print(f"β… Loaded: {model_name}")
    return tokenizer, model


def generate_with_hf(
    prompt: str,
    model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
    max_new_tokens: int = 256,
    temperature: float = 0.1
) -> str:
    """HuggingFace μƒμ„±"""
    tokenizer, model = load_llm(model_name)

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            repetition_penalty=1.15,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )

    return tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):],
        skip_special_tokens=True
    ).strip()


# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•
# ν†µν•© μΈν„°νμ΄μ¤
# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•

def get_llm_response(
    prompt: str,
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama",
    max_tokens: int = 256,
    temperature: float = 0.1
) -> str:
    """ν†µν•© LLM μ‘λ‹µ"""
    if llm_backend == "ollama":
        llm = OllamaLLM(llm_model)
        return llm.generate(prompt, temperature=temperature, max_tokens=max_tokens)
    else:
        return generate_with_hf(prompt, llm_model, max_tokens, temperature)


# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•
# κ²€μƒ‰ κ²°κ³Ό λ¶„μ„ (μ—μ΄μ „νΈμ©)
# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•

def analyze_search_results(results: List[Dict]) -> Dict:
    """κ²€μƒ‰ κ²°κ³Ό λ¶„μ„ - λλ¬»κΈ° μ—¬λ¶€ νλ‹¨"""
    if not results:
        return {'needs_clarification': False, 'options': [], 'unique_documents': []}

    doc_groups = {}

    for r in results:
        meta = r.get('metadata', {})
        doc_name = meta.get('doc_name', 'unknown')
        doc_title = meta.get('doc_title', doc_name)
        article_num = meta.get('article_num')
        article_type = meta.get('article_type', 'article')
        section = meta.get('section')  # μ Nμ΅° ν•μ‹
        score = r.get('similarity', 0)

        # μ„Ήμ… ν‘μ‹
        section_display = section or ""
        if not section_display and article_num:
            if article_type == 'article':
                section_display = f"μ {article_num}μ΅°"
            elif article_type == 'chapter':
                section_display = f"μ {article_num}μ¥"
            elif article_type == 'section':
                section_display = f"μ {article_num}μ "
            else:
                section_display = str(article_num)

        if doc_name not in doc_groups:
            doc_groups[doc_name] = {
                'title': doc_title,
                'max_score': score,
                'sections': {section_display} if section_display else set(),
                'count': 1
            }
        else:
            doc_groups[doc_name]['max_score'] = max(doc_groups[doc_name]['max_score'], score)
            if section_display:
                doc_groups[doc_name]['sections'].add(section_display)
            doc_groups[doc_name]['count'] += 1

    unique_docs = list(doc_groups.keys())

    # λλ¬»κΈ° νλ³„: μ—¬λ¬ λ¬Έμ„μ—μ„ λΉ„μ·ν• μ μ
    needs_clarification = False
    if len(unique_docs) > 1:
        scores = sorted([info['max_score'] for info in doc_groups.values()], reverse=True)
        if len(scores) >= 2 and (scores[0] - scores[1]) < 0.15:
            needs_clarification = True

    # μ„ νƒμ§€
    options = []
    for d_name in unique_docs:
        info = doc_groups[d_name]
        sections_list = sorted(list(info['sections']))[:3]
        sections_str = f" ({', '.join(sections_list)})" if sections_list else ""

        options.append({
            "doc_name": d_name,
            "doc_title": info['title'],
            "display_text": f"{info['title']}{sections_str}",
            "score": info['max_score'],
            "sections": sections_list,
        })

    return {
        'needs_clarification': needs_clarification,
        'options': options,
        'unique_documents': unique_docs
    }


def generate_clarification_question(
    query: str,
    options: List[Dict],
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama"
) -> str:
    """λλ¬»κΈ° μ§λ¬Έ μƒμ„±"""
    options_text = "\n".join([
        f"- {opt['display_text']} (κ΄€λ ¨λ„: {opt['score']:.0%})"
        for opt in options
    ])

    prompt = f"""μ‚¬μ©μκ°€ "{query}"μ— λ€ν•΄ μ§λ¬Έν–μµλ‹λ‹¤.
κ΄€λ ¨ λ¬Έμ„λ“¤:
{options_text}

μ–΄λ–¤ λ¬Έμ„μ λ‚΄μ©μ„ λ°”νƒ•μΌλ΅ λ‹µλ³€ν• μ§€ μ •μ¤‘ν•κ² λ¬Όμ–΄λ³΄μ„Έμ”.
ν•κµ­μ–΄λ΅ μ§§κ² μ‘λ‹µν•μ„Έμ”."""

    try:
        return get_llm_response(prompt, llm_model, llm_backend, max_tokens=200)
    except Exception:
        return f"'{query}'μ— λ€ν•΄ μ—¬λ¬ κ·μ •(SOP)μ΄ κ²€μƒ‰λμ—μµλ‹λ‹¤. μ–΄λ–¤ λ¬Έμ„λ¥Ό ν™•μΈν• κΉμ”?\n\n{options_text}"


# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•
# λ¨λΈ ν”„λ¦¬μ…‹
# β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•β•

OLLAMA_MODELS = [
    {"key": "qwen2.5:0.5b", "name": "Qwen2.5-0.5B", "desc": "μ΄κ²½λ‰ (1GB)", "vram": "1GB"},
    {"key": "qwen2.5:1.5b", "name": "Qwen2.5-1.5B", "desc": "κ²½λ‰ (2GB)", "vram": "2GB"},
    {"key": "qwen2.5:3b", "name": "Qwen2.5-3B", "desc": "μ¶”μ² (3GB)", "vram": "3GB"},
    {"key": "qwen2.5:7b", "name": "Qwen2.5-7B", "desc": "κ³ μ„±λ¥ (5GB)", "vram": "5GB"},
    {"key": "qwen3:4b", "name": "Qwen3-4B", "desc": "μµμ‹  (4GB)", "vram": "4GB"},
    {"key": "llama3.2:3b", "name": "Llama3.2-3B", "desc": "κ²½λ‰ (3GB)", "vram": "3GB"},
    {"key": "gemma2:2b", "name": "Gemma2-2B", "desc": "κ²½λ‰ (2GB)", "vram": "2GB"},
    {"key": "mistral:7b", "name": "Mistral-7B", "desc": "μμ–΄ νΉν™” (5GB)", "vram": "5GB"},
]

HUGGINGFACE_MODELS = [
    {"key": "Qwen/Qwen2.5-0.5B-Instruct", "name": "Qwen2.5-0.5B", "desc": "μ΄κ²½λ‰"},
    {"key": "Qwen/Qwen2.5-1.5B-Instruct", "name": "Qwen2.5-1.5B", "desc": "κ²½λ‰"},
    {"key": "Qwen/Qwen2.5-3B-Instruct", "name": "Qwen2.5-3B", "desc": "VRAM 6GB+"},
    {"key": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "name": "TinyLlama", "desc": "μμ–΄"},
]