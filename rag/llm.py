"""
LLM ëª¨ë“ˆ - HuggingFace + Ollama ì§€ì›
"""

import torch
import requests
from typing import Dict, List, Optional, Any, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"

_loaded_llm: Dict[str, Any] = {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ollama ë°±ì—”ë“œ (ë¡œì»¬ ì¶”ì²œ) - ë„¤ì´í‹°ë¸Œ API ì‚¬ìš©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OllamaLLM:
    """Ollama ë„¤ì´í‹°ë¸Œ API ì‚¬ìš© (chat/generate ìë™ ì„ íƒ)"""

    def __init__(self, model: str = "qwen2.5:3b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url

    def generate(self, prompt: str, system: str = None, temperature: float = 0.1, max_tokens: int = 256) -> str:
        """Ollama API í˜¸ì¶œ - /api/chat ì‹œë„ í›„ ì‹¤íŒ¨ì‹œ /api/generate ì‚¬ìš©"""
        
        print(f"ğŸ¤– Ollama í˜¸ì¶œ: model={self.model}, prompt ê¸¸ì´={len(prompt)}")
        
        # 1ì°¨ ì‹œë„: /api/chat (ìµœì‹  Ollama)
        try:
            return self._call_chat_api(prompt, system, temperature, max_tokens)
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                print(f"âš ï¸ /api/chat 404 - /api/generateë¡œ fallback")
                # /api/chatì´ ì—†ìœ¼ë©´ /api/generate ì‹œë„
                return self._call_generate_api(prompt, system, temperature, max_tokens)
            raise
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                f"'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  "
                f"'ollama pull {self.model}' ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
    
    def _call_chat_api(self, prompt: str, system: str, temperature: float, max_tokens: int) -> str:
        """/api/chat ì—”ë“œí¬ì¸íŠ¸ (Ollama 0.1.14+)"""
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        
        # Qwen3 ëª¨ë¸ì€ thinking ëª¨ë“œê°€ ê¸°ë³¸ â†’ /no_thinkë¡œ ë¹„í™œì„±í™”
        final_prompt = prompt
        if "qwen3" in self.model.lower():
            final_prompt = f"/no_think {prompt}"
        
        messages.append({"role": "user", "content": final_prompt})
        
        try:
            # Qwen3ìš© ì¶”ê°€ ì˜µì…˜
            options = {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "options": options,
                    "think": False,  # Qwen3 thinking ëª¨ë“œ ë„ê¸°
                },
                timeout=120
            )
            print(f"ğŸ“¡ Ollama /api/chat ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            
            if not response.ok:
                print(f"âŒ Ollama ì—ëŸ¬ ì‘ë‹µ: {response.text}")
            
            response.raise_for_status()
            data = response.json()
            print(f"âœ… Ollama ì‘ë‹µ í‚¤: {data.keys()}")
            
            message = data.get("message", {})
            content = message.get("content", "") if isinstance(message, dict) else str(message)
            
            # Qwen3 thinking ëª¨ë“œ: contentê°€ ë¹„ì–´ìˆìœ¼ë©´ thinking ì‚¬ìš©
            if not content and isinstance(message, dict):
                thinking = message.get("thinking", "")
                if thinking:
                    print(f"ğŸ§  thinking ëª¨ë“œ ê°ì§€ - thinking ë‚´ìš© ì‚¬ìš©")
                    # thinkingì—ì„œ ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ë³´í†µ ë‹µë³€)
                    content = thinking
            
            print(f"ğŸ“ ìµœì¢… content ê¸¸ì´: {len(content)}")
            
            if not content:
                print(f"âš ï¸ ë¹ˆ ì‘ë‹µ! ì „ì²´ data: {data}")
            
            return content
        except requests.exceptions.HTTPError:
            raise
        except Exception as e:
            print(f"âŒ Ollama chat API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            raise
    
    def _call_generate_api(self, prompt: str, system: str, temperature: float, max_tokens: int) -> str:
        """/api/generate ì—”ë“œí¬ì¸íŠ¸ (êµ¬ë²„ì „ í˜¸í™˜)"""
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                    }
                },
                timeout=120
            )
            print(f"ğŸ“¡ Ollama /api/generate ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            
            if not response.ok:
                print(f"âŒ Ollama ì—ëŸ¬ ì‘ë‹µ: {response.text}")
            
            response.raise_for_status()
            data = response.json()
            print(f"âœ… Ollama ì‘ë‹µ í‚¤: {data.keys()}")
            return data.get("response", "")
        except Exception as e:
            print(f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            raise

    @staticmethod
    def list_models(base_url: str = "http://localhost:11434") -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.ok:
                data = response.json()
                models = data.get("models", [])
                return [m["name"] for m in models]
        except Exception as e:
            print(f"âš ï¸ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

    @staticmethod
    def is_available(base_url: str = "http://localhost:11434") -> bool:
        """Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€"""
        try:
            # /api/tagsë¡œ í™•ì¸
            response = requests.get(f"{base_url}/api/tags", timeout=3)
            if response.ok:
                return True
            # fallback: ë£¨íŠ¸ ê²½ë¡œ í™•ì¸
            response = requests.get(base_url, timeout=3)
            return response.ok
        except:
            return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HuggingFace ë°±ì—”ë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_llm(model_name: str):
    """HuggingFace LLM ë¡œë“œ (ìºì‹±)"""
    if model_name in _loaded_llm:
        return _loaded_llm[model_name]

    print(f"ğŸ¤– Loading LLM: {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    
    # dtype ì„¤ì •
    dtype = torch.float16 if device == "cuda" else torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map="auto" if device == "cuda" else None
    )
    
    if device == "cpu":
        model = model.to(device)
    
    model.eval()

    _loaded_llm[model_name] = (tokenizer, model)
    print(f"âœ… LLM loaded: {model_name}")
    return tokenizer, model


def generate_with_hf(
    prompt: str,
    model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
    max_new_tokens: int = 256,
    temperature: float = 0.1
) -> str:
    """HuggingFace ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
    tokenizer, model = load_llm(model_name)
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            repetition_penalty=1.15,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )
    
    decoded = tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):],
        skip_special_tokens=True
    )
    
    return decoded.strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© LLM ì¸í„°í˜ì´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_llm_response(
    prompt: str,
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama",
    max_tokens: int = 256,
    temperature: float = 0.1
) -> str:
    """
    í†µí•© LLM ì‘ë‹µ ìƒì„±
    
    Args:
        prompt: í”„ë¡¬í”„íŠ¸
        llm_model: ëª¨ë¸ëª…
        llm_backend: 'ollama' ë˜ëŠ” 'huggingface'
        max_tokens: ìµœëŒ€ í† í°
        temperature: ì˜¨ë„
    """
    if llm_backend == "ollama":
        llm = OllamaLLM(llm_model)
        return llm.generate(prompt, temperature=temperature, max_tokens=max_tokens)
    else:
        return generate_with_hf(prompt, llm_model, max_tokens, temperature)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì—ì´ì „íŠ¸ í•µì‹¬: ë˜ë¬»ê¸° ë¶„ì„ ë° ì„¹ì…˜ ì¶”ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_search_results(results: List[Dict]) -> Dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ì˜ ë©”íƒ€ë°ì´í„°(ì¡°í•­, ì„¹ì…˜)ë¥¼ ë¶„ì„í•˜ì—¬ ë˜ë¬»ê¸° ì—¬ë¶€ íŒë‹¨
    """
    if not results:
        return {'needs_clarification': False, 'options': [], 'unique_documents': []}

    doc_groups = {}

    for r in results:
        meta = r.get('metadata', {})
        doc_name = meta.get('doc_name', 'unknown')
        doc_title = meta.get('doc_title', doc_name)
        article_num = meta.get('article_num')
        article_type = meta.get('article_type', 'article')
        score = r.get('similarity', 0)
        
        # í‘œì‹œìš© ì„¹ì…˜ ì´ë¦„ ìƒì„±
        section_display = ""
        if article_num:
            if article_type == 'article': 
                section_display = f"ì œ{article_num}ì¡°"
            elif article_type == 'chapter': 
                section_display = f"ì œ{article_num}ì¥"
            else: 
                section_display = f"{article_num}"

        if doc_name not in doc_groups:
            doc_groups[doc_name] = {
                'title': doc_title,
                'max_score': score,
                'sections': {section_display} if section_display else set(),
                'count': 1
            }
        else:
            doc_groups[doc_name]['max_score'] = max(doc_groups[doc_name]['max_score'], score)
            if section_display:
                doc_groups[doc_name]['sections'].add(section_display)
            doc_groups[doc_name]['count'] += 1

    unique_docs = list(doc_groups.keys())
    
    # ë˜ë¬»ê¸° íŒë³„ ë¡œì§
    needs_clarification = False
    if len(unique_docs) > 1:
        scores = sorted([info['max_score'] for info in doc_groups.values()], reverse=True)
        if len(scores) >= 2 and (scores[0] - scores[1]) < 0.15:
            needs_clarification = True

    # ì„ íƒì§€ ë°ì´í„° êµ¬ì„±
    clarification_options = []
    for d_name in unique_docs:
        info = doc_groups[d_name]
        sections_list = sorted(list(info['sections']))
        sections_str = f" ({', '.join(sections_list[:2])})" if sections_list else ""
        
        clarification_options.append({
            "doc_name": d_name,
            "doc_title": info['title'],
            "display_text": f"{info['title']}{sections_str}",
            "score": info['max_score']
        })

    return {
        'needs_clarification': needs_clarification,
        'options': clarification_options,
        'unique_documents': unique_docs
    }


def generate_clarification_question(
    query: str, 
    options: List[Dict],
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama"
) -> str:
    """
    ì„¹ì…˜ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë˜ì§ˆ ì§ˆë¬¸ ìƒì„±
    """
    options_text = "\n".join([f"- {opt['display_text']}" for opt in options])
    
    prompt = f"""ì‚¬ìš©ìê°€ "{query}"ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ íŠ¹ì • ì¡°í•­ë“¤ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤:
{options_text}

ì‚¬ìš©ìì—ê²Œ ì–´ë–¤ ë¬¸ì„œ(SOP)ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ë“œë¦´ì§€ ì •ì¤‘í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.
ê²€ìƒ‰ëœ ì¡°í•­ë“¤(ì˜ˆ: ì œXì¡°)ì„ ì–¸ê¸‰í•˜ì—¬ ì „ë¬¸ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”. 
ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ í•˜ì„¸ìš”."""
    
    try:
        return get_llm_response(prompt, llm_model, llm_backend, max_tokens=200)
    except:
        return f"'{query}'ì— ëŒ€í•´ ì—¬ëŸ¬ ê·œì •(SOP)ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì„œì˜ ë‚´ìš©ì„ í™•ì¸í•´ ë“œë¦´ê¹Œìš”?\n\n" + options_text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìµœì¢… ë‹µë³€ ìƒì„± (RAG)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_answer_with_context(
    query: str,
    context: str,
    llm_model: str = "qwen2.5:3b",
    llm_backend: str = "ollama"
) -> str:
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±"""
    prompt = f"""ë‹¹ì‹ ì€ ê·œì •(SOP) ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ì°¸ê³  ë¬¸ì„œ]ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. ë‹µë³€ ì‹œ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­(ì˜ˆ: ì œNì¡°)ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì „ë¬¸ê°€ ë‹µë³€]:"""
    
    return get_llm_response(prompt, llm_model, llm_backend, max_tokens=1024, temperature=0.2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¨ë¸ í”„ë¦¬ì…‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OLLAMA_MODELS = [
    {"key": "qwen2.5:0.5b", "name": "Qwen2.5-0.5B", "desc": "ì´ˆê²½ëŸ‰ (1GB)", "vram": "1GB"},
    {"key": "qwen2.5:1.5b", "name": "Qwen2.5-1.5B", "desc": "ê²½ëŸ‰ (2GB)", "vram": "2GB"},
    {"key": "qwen2.5:3b", "name": "Qwen2.5-3B", "desc": "ì¶”ì²œ (3GB)", "vram": "3GB"},
    {"key": "qwen2.5:7b", "name": "Qwen2.5-7B", "desc": "ê³ ì„±ëŠ¥ (5GB)", "vram": "5GB"},
    {"key": "qwen3:4b", "name": "Qwen3-4B", "desc": "ìµœì‹  ì¶”ì²œ (4GB)", "vram": "4GB"},
    {"key": "llama3.2:3b", "name": "Llama3.2-3B", "desc": "ê²½ëŸ‰ (3GB)", "vram": "3GB"},
    {"key": "gemma2:2b", "name": "Gemma2-2B", "desc": "ê²½ëŸ‰ (2GB)", "vram": "2GB"},
    {"key": "gemma2:9b", "name": "Gemma2-9B", "desc": "ê³ ì„±ëŠ¥ (6GB)", "vram": "6GB"},
    {"key": "mistral:7b", "name": "Mistral-7B", "desc": "ì˜ì–´ íŠ¹í™” (5GB)", "vram": "5GB"},
]

HUGGINGFACE_MODELS = [
    {"key": "Qwen/Qwen2.5-0.5B-Instruct", "name": "Qwen2.5-0.5B", "desc": "ì´ˆê²½ëŸ‰"},
    {"key": "Qwen/Qwen2.5-1.5B-Instruct", "name": "Qwen2.5-1.5B", "desc": "ê²½ëŸ‰"},
    {"key": "Qwen/Qwen2.5-3B-Instruct", "name": "Qwen2.5-3B", "desc": "VRAM 6GB+"},
    {"key": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "name": "TinyLlama", "desc": "ì˜ì–´ íŠ¹í™”"},
]


