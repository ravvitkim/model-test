"""
í…ìŠ¤íŠ¸ ì²­í‚¹ ëª¨ë“ˆ v6.2 - section_path ì§€ì›
- sentence: ë¬¸ì¥ ë‹¨ìœ„
- paragraph: ë¬¸ë‹¨ ë‹¨ìœ„
- article: ì¡°í•­ ë‹¨ìœ„ (SOP/ë²•ë¥ )
- recursive: RecursiveCharacterTextSplitter
- semantic: ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
- llm: LLM ê¸°ë°˜ êµ¬ì¡° íŒŒì‹±

ğŸ”¥ v6.2 ì¶”ê°€:
- section_path: "5 > 5.1 > 5.1.1" í˜•íƒœì˜ ê³„ì¸µ ê²½ë¡œ ì§€ì›
- section_path_readable: ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœ
"""

import re
from typing import List, Optional, Callable, Dict
from dataclasses import dataclass, field
import numpy as np


@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„°"""
    text: str
    index: int = 0
    metadata: Dict = field(default_factory=dict)


CHUNK_METHODS = {
    "sentence": "ë¬¸ì¥ ë‹¨ìœ„",
    "paragraph": "ë¬¸ë‹¨ ë‹¨ìœ„",
    "article": "ì¡°í•­ ë‹¨ìœ„ (SOP/ë²•ë¥ )",
    "recursive": "Recursive (ë­ì²´ì¸ ìŠ¤íƒ€ì¼)",
    "semantic": "ì˜ë¯¸ ê¸°ë°˜",
    "llm": "LLM íŒŒì‹±",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¡°í•­ íŒ¨í„´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ARTICLE_PATTERNS = [
    (r'^ì œ\s*(\d+)\s*ì¡°', 'article'),
    (r'^ì œ\s*(\d+)\s*ì¥', 'chapter'),
    (r'^ì œ\s*(\d+)\s*ì ˆ', 'section'),
    (r'^(\d+)\.\s+([ê°€-í£A-Za-z].+)', 'section'),       # "1. ëª©ì " í˜•ì‹
    (r'^(\d+\.\d+)\s+([ê°€-í£A-Za-z].+)', 'subsection'), # "6.1 ì‚¬ì „ ì¤€ë¹„" í˜•ì‹
    (r'^(\d+\.\d+\.\d+)\s+([ê°€-í£A-Za-z].+)', 'subsubsection'), # "5.1.1 Level 1" í˜•ì‹
]


def detect_article(line: str) -> Optional[tuple]:
    """ì¡°í•­ ê°ì§€"""
    line = line.strip()
    for pattern, a_type in ARTICLE_PATTERNS:
        match = re.match(pattern, line)
        if match:
            return (match.group(1), a_type)
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. ë¬¸ì¥ ë‹¨ìœ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_by_sentences(text: str, max_length: int = 300, overlap: int = 50) -> List[str]:
    """ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• """
    sentences = re.split(r'(?<=[.!?ã€‚])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sent_len = len(sentence)

        if current_length + sent_len > max_length and current_chunk:
            chunks.append(' '.join(current_chunk))

            # ì˜¤ë²„ë©
            overlap_sentences = []
            temp_len = 0
            for s in reversed(current_chunk):
                if temp_len + len(s) <= overlap:
                    overlap_sentences.insert(0, s)
                    temp_len += len(s)
                else:
                    break
            current_chunk = overlap_sentences
            current_length = sum(len(s) for s in current_chunk)

        current_chunk.append(sentence)
        current_length += sent_len

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ë¬¸ë‹¨ ë‹¨ìœ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_by_paragraphs(text: str, max_length: int = 1000) -> List[str]:
    """ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• """
    paragraphs = re.split(r'\n\s*\n', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip()]

    chunks = []
    current_chunk = []
    current_length = 0

    for para in paragraphs:
        para_len = len(para)

        if para_len > max_length:
            if current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_length = 0
            chunks.extend(split_by_sentences(para, max_length))

        elif current_length + para_len > max_length:
            chunks.append('\n\n'.join(current_chunk))
            current_chunk = [para]
            current_length = para_len

        else:
            current_chunk.append(para)
            current_length += para_len

    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))

    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ì¡°í•­ ë‹¨ìœ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def split_by_articles(text: str, max_length: int = 500, overlap: int = 50) -> List[dict]:
    """ì¡°í•­ ë‹¨ìœ„ ë¶„í• """
    lines = text.split('\n')
    articles = []

    current = {'lines': [], 'article_num': None, 'article_type': 'intro'}

    def flush():
        if current['lines']:
            article_text = '\n'.join(current['lines']).strip()
            if article_text:
                articles.append({
                    'text': article_text,
                    'article_num': current['article_num'],
                    'article_type': current['article_type'],
                })

    for line in lines:
        detected = detect_article(line)
        if detected:
            flush()
            current = {
                'lines': [line],
                'article_num': detected[0],
                'article_type': detected[1],
            }
        else:
            current['lines'].append(line)

    flush()

    # ê¸´ ì¡°í•­ ë¶„í• 
    result = []
    for art in articles:
        if len(art['text']) > max_length:
            sub_texts = split_by_sentences(art['text'], max_length, overlap)
            for i, sub in enumerate(sub_texts):
                result.append({
                    **art,
                    'text': sub,
                    'chunk_part': f"{i+1}/{len(sub_texts)}"
                })
        else:
            result.append(art)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Recursive Character Text Splitter
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RecursiveCharacterTextSplitter:
    """ë­ì²´ì¸ ìŠ¤íƒ€ì¼ Recursive ë¶„í• ê¸°"""

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", ". ", " ", ""]

    def split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ë¶„í• """
        return self._split_recursive(text, self.separators)

    def _split_recursive(self, text: str, separators: List[str]) -> List[str]:
        if not text:
            return []

        if len(text) <= self.chunk_size:
            return [text]

        if not separators:
            return self._split_by_size(text)

        sep = separators[0]
        remaining_seps = separators[1:]

        if sep:
            parts = text.split(sep)
        else:
            return self._split_by_size(text)

        chunks = []
        current_chunk = []
        current_size = 0

        for part in parts:
            part_with_sep = part + sep if sep else part
            part_size = len(part_with_sep)

            if part_size > self.chunk_size:
                if current_chunk:
                    chunks.append(sep.join(current_chunk))
                    current_chunk = []
                    current_size = 0
                chunks.extend(self._split_recursive(part, remaining_seps))

            elif current_size + part_size > self.chunk_size:
                if current_chunk:
                    chunks.append(sep.join(current_chunk))

                    # ì˜¤ë²„ë©
                    overlap_parts = []
                    overlap_size = 0
                    for p in reversed(current_chunk):
                        if overlap_size + len(p) <= self.chunk_overlap:
                            overlap_parts.insert(0, p)
                            overlap_size += len(p)
                        else:
                            break
                    current_chunk = overlap_parts
                    current_size = sum(len(p) for p in current_chunk)

                current_chunk.append(part)
                current_size += part_size
            else:
                current_chunk.append(part)
                current_size += part_size

        if current_chunk:
            chunks.append(sep.join(current_chunk))

        return chunks

    def _split_by_size(self, text: str) -> List[str]:
        """í¬ê¸° ê¸°ë°˜ ë¶„í• """
        chunks = []
        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunks.append(text[i:i + self.chunk_size])
        return chunks


def split_recursive(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """Recursive ë¶„í•  í—¬í¼"""
    splitter = RecursiveCharacterTextSplitter(chunk_size, overlap)
    return splitter.split_text(text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. Semantic ë¶„í• 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SemanticSplitter:
    """ì˜ë¯¸ ê¸°ë°˜ ë¶„í• ê¸°"""

    def __init__(
        self,
        embed_function: Callable[[str], List[float]],
        threshold: float = 0.5,
        max_chunk_size: int = 500
    ):
        self.embed_function = embed_function
        self.threshold = threshold
        self.max_chunk_size = max_chunk_size

    def split_text(self, text: str) -> List[str]:
        """ì˜ë¯¸ ê¸°ë°˜ ë¶„í• """
        sentences = re.split(r'(?<=[.!?ã€‚])\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) <= 1:
            return sentences

        # ì„ë² ë”© ê³„ì‚°
        embeddings = [self.embed_function(s) for s in sentences]

        # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„í• ì  ì°¾ê¸°
        split_points = [0]
        for i in range(1, len(embeddings)):
            sim = self._cosine_similarity(embeddings[i-1], embeddings[i])
            if sim < self.threshold:
                split_points.append(i)
        split_points.append(len(sentences))

        # ì²­í¬ ìƒì„±
        chunks = []
        for i in range(len(split_points) - 1):
            start, end = split_points[i], split_points[i+1]
            chunk_text = ' '.join(sentences[start:end])

            if len(chunk_text) > self.max_chunk_size:
                chunks.extend(split_recursive(chunk_text, self.max_chunk_size))
            else:
                chunks.append(chunk_text)

        return chunks

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        a, b = np.array(a), np.array(b)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))


def split_semantic(
    text: str,
    embed_function: Callable = None,
    threshold: float = 0.5,
    max_chunk_size: int = 500
) -> List[str]:
    """Semantic ë¶„í•  í—¬í¼"""
    if embed_function is None:
        return split_recursive(text, max_chunk_size)

    splitter = SemanticSplitter(embed_function, threshold, max_chunk_size)
    return splitter.split_text(text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. LLM ê¸°ë°˜ íŒŒì‹±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LLM_PARSING_PROMPT = """ë‹¤ìŒ ë¬¸ì„œì˜ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ë¬¸ì„œ:
{text}

JSON í˜•ì‹ (ë°°ì—´ë§Œ ë°˜í™˜):
[{{"title": "ì„¹ì…˜ì œëª©", "start": ì‹œì‘ì¸ë±ìŠ¤, "end": ëì¸ë±ìŠ¤}}, ...]"""


def split_by_llm(
    text: str,
    llm_function: Callable[[str], str] = None,
    max_chunk_size: int = 500,
    fallback_method: str = "recursive"
) -> List[dict]:
    """LLM ê¸°ë°˜ ë¶„í• """
    if llm_function is None:
        return _fallback_split(text, max_chunk_size, fallback_method)

    analysis_text = text[:8000] if len(text) > 8000 else text

    try:
        import json

        prompt = LLM_PARSING_PROMPT.format(text=analysis_text)
        response = llm_function(prompt)

        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if not json_match:
            raise ValueError("JSON not found")

        sections = json.loads(json_match.group())

        chunks = []
        for i, section in enumerate(sections):
            start = section.get('start', 0)
            end = section.get('end', len(text))
            section_text = text[start:end].strip()

            if len(section_text) > max_chunk_size:
                sub_chunks = split_recursive(section_text, max_chunk_size)
                for j, sub in enumerate(sub_chunks):
                    chunks.append({
                        'text': sub,
                        'title': section.get('title', f'Section {i+1}'),
                        'section_index': i,
                        'sub_index': j,
                        'parse_method': 'llm'
                    })
            else:
                chunks.append({
                    'text': section_text,
                    'title': section.get('title', f'Section {i+1}'),
                    'section_index': i,
                    'parse_method': 'llm'
                })

        return chunks if chunks else _fallback_split(text, max_chunk_size, fallback_method)

    except Exception as e:
        print(f"âš ï¸ LLM íŒŒì‹± ì‹¤íŒ¨: {e}")
        return _fallback_split(text, max_chunk_size, fallback_method)


def _fallback_split(text: str, max_size: int, method: str) -> List[dict]:
    """Fallback ë¶„í• """
    if method == "recursive":
        raw = split_recursive(text, max_size)
    elif method == "sentence":
        raw = split_by_sentences(text, max_size)
    else:
        raw = split_by_paragraphs(text, max_size)

    return [{'text': c, 'parse_method': f'fallback_{method}'} for c in raw]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© ì¸í„°í˜ì´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_chunks(
    text: str,
    chunk_size: int = 500,
    overlap: int = 50,
    method: str = "sentence",
    embed_function: Callable = None,
    llm_function: Callable = None,
    semantic_threshold: float = 0.5,
) -> List[Chunk]:
    """í†µí•© ì²­í‚¹ í•¨ìˆ˜"""

    if method == "article":
        raw = split_by_articles(text, chunk_size, overlap)
        return [
            Chunk(
                text=r['text'].strip(),
                index=i,
                metadata={
                    "article_num": r.get('article_num'),
                    "article_type": r.get('article_type'),
                    "chunk_part": r.get('chunk_part'),
                }
            )
            for i, r in enumerate(raw) if r['text'].strip()
        ]

    elif method == "recursive":
        raw = split_recursive(text, chunk_size, overlap)

    elif method == "semantic":
        raw = split_semantic(text, embed_function, semantic_threshold, chunk_size)

    elif method == "llm":
        raw_data = split_by_llm(text, llm_function, chunk_size)
        return [
            Chunk(
                text=r['text'].strip(),
                index=i,
                metadata={"title": r.get('title'), "parse_method": r.get('parse_method')}
            )
            for i, r in enumerate(raw_data) if r['text'].strip()
        ]

    elif method == "paragraph":
        raw = split_by_paragraphs(text, chunk_size)

    else:  # sentence
        raw = split_by_sentences(text, chunk_size, overlap)

    return [
        Chunk(text=c.strip(), index=i, metadata={})
        for i, c in enumerate(raw) if c.strip()
    ]


def create_chunks_from_blocks(
    doc,  # ParsedDocument
    chunk_size: int = 500,
    overlap: int = 50,
    method: str = "recursive"
) -> List[Chunk]:
    """
    ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ (ë©”íƒ€ë°ì´í„° ìœ ì§€, ì„¹ì…˜ë³„ SOP ID)
    
    ğŸ”¥ v6.2: section_path, section_path_readable ì¶”ê°€
    """
    chunks = []
    idx = 0

    for block in doc.blocks:
        if method == "recursive":
            texts = split_recursive(block.text, chunk_size, overlap)
        else:
            texts = [block.text]

        for t in texts:
            if not t.strip():
                continue

            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            article_num = block.metadata.get('article_num') or block.section
            article_type = block.metadata.get('article_type', 'article')
            
            # ë¸”ë¡ë³„ SOP ID ìš°ì„ , ì—†ìœ¼ë©´ ë¬¸ì„œ ì „ì²´ SOP ID
            sop_id = block.metadata.get('sop_id') or doc.metadata.get("sop_id")

            # ê°€ë…ì„± ì¢‹ì€ section í‘œì‹œ
            section_display = None
            if article_num:
                if article_type == 'article':
                    section_display = f"ì œ{article_num}ì¡°"
                elif article_type == 'chapter':
                    section_display = f"ì œ{article_num}ì¥"
                elif article_type == 'section':
                    section_display = article_num  # "1", "6" ë“±
                elif article_type == 'subsection':
                    section_display = article_num  # "6.1", "6.2" ë“±
                elif article_type == 'subsubsection':
                    section_display = article_num  # "5.1.1" ë“±
                else:
                    section_display = str(article_num)

            # ğŸ”¥ section_path ì¶”ê°€
            section_path = block.metadata.get("section_path")
            section_path_readable = block.metadata.get("section_path_readable")

            chunks.append(Chunk(
                text=t.strip(),
                index=idx,
                metadata={
                    "doc_name": doc.metadata.get("file_name"),
                    "doc_title": doc.metadata.get("title"),
                    "sop_id": sop_id,  # ì„¹ì…˜ë³„ SOP ID
                    "version": doc.metadata.get("version"),
                    "article_num": article_num,
                    "article_type": article_type,
                    "section": section_display,
                    "section_path": section_path,                    # ğŸ”¥ ì¶”ê°€
                    "section_path_readable": section_path_readable,  # ğŸ”¥ ì¶”ê°€
                    "title": block.metadata.get("title"),  # ì¡°í•­ ì œëª©
                    "page": block.page,
                    "block_type": block.block_type,
                }
            ))
            idx += 1

    return chunks


def get_available_methods() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í‚¹ ë°©ë²•"""
    return CHUNK_METHODS.copy()