"""
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ API - ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì› + RAG + Ollama + ì—ì´ì „íŠ¸
v5.0 - í™•ì¥ ì²­í‚¹ + ì„ë² ë”© ëª¨ë¸ í•„í„°ë§
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import re
import time

# RAG ëª¨ë“ˆ
from rag.document_loader import load_document, get_supported_extensions
from rag.chunker import (
    create_chunks, 
    get_available_methods,
    CHUNK_METHODS,
    split_semantic,
    split_by_llm,
)
from rag import vector_store
from rag.prompt import build_rag_prompt, build_chunk_prompt
from rag.llm import (
    load_llm, 
    get_llm_response,
    OllamaLLM,
    analyze_search_results,
    generate_clarification_question,
    OLLAMA_MODELS,
    HUGGINGFACE_MODELS
)


app = FastAPI(title="Text Similarity + RAG API", version="5.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í”„ë¦¬ì…‹ ëª¨ë¸ (ë¹ ë¥¸ ì„ íƒìš©) - í˜¸í™˜ ëª¨ë¸ë§Œ í¬í•¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRESET_MODELS = {
    # í•œêµ­ì–´ ì „ìš© (ê¶Œì¥)
    "ko-sroberta": "jhgan/ko-sroberta-multitask",
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    
    # ë‹¤êµ­ì–´
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5": "intfloat/multilingual-e5-large",
    "bge-m3": "BAAI/bge-m3",
    
    # ì˜ì–´ ì „ìš©
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    
    # Qwen Embedding (í˜¸í™˜ ëª¨ë¸ë§Œ)
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
    # "qwen3-4b" ì œê±° (dim=2560, mem=4GB ì´ˆê³¼)
}

# ì „ì—­ ë³€ìˆ˜
loaded_models = {}
device = "cuda" if torch.cuda.is_available() else "cpu"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CompareRequest(BaseModel):
    text1: str
    text2: str
    model: str = "ko-sroberta"

class MultiModelCompareRequest(BaseModel):
    text1: str
    text2: str
    models: List[str]

class MatrixRequest(BaseModel):
    texts: List[str]
    model: str = "ko-sroberta"

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = 5
    model: str = "ko-sroberta"
    filter_doc: Optional[str] = None

class AskRequest(BaseModel):
    """ì—ì´ì „íŠ¸ íŒ¨í„´ RAG ìš”ì²­"""
    query: str
    collection: str = "documents"
    n_results: int = 5
    embedding_model: str = "ko-sroberta"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    check_clarification: bool = True
    filter_doc: Optional[str] = None

class AskChunkRequest(BaseModel):
    query: str
    chunk_text: str
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"

class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"

class EmbeddingFilterRequest(BaseModel):
    """ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ ìš”ì²­"""
    max_dim: int = Field(default=1024, description="ìµœëŒ€ ì„ë² ë”© ì°¨ì›")
    max_memory_mb: int = Field(default=1300, description="ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model_key: str) -> str:
    """í”„ë¦¬ì…‹ í‚¤ë©´ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜"""
    return PRESET_MODELS.get(model_key, model_key)


def load_model(model_key: str):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    model_path = resolve_model_path(model_key)
    
    if model_path in loaded_models:
        return loaded_models[model_path], 0.0
    
    print(f"ğŸ“¦ Loading embedding model: {model_path}...")
    start_time = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)
    model.eval()
    
    load_time = time.time() - start_time
    loaded_models[model_path] = (tokenizer, model)
    print(f"âœ… Embedding model loaded: {model_path} ({load_time:.2f}s)")
    
    return (tokenizer, model), load_time


def embed_text(text: str, tokenizer, model) -> np.ndarray:
    """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    MAX_TEXT_LENGTH = 1500
    if len(text) > MAX_TEXT_LENGTH:
        text = text[:MAX_TEXT_LENGTH]
    
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    attention_mask = inputs['attention_mask']
    token_embeddings = outputs.last_hidden_state
    
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    embedding = (sum_embeddings / sum_mask).cpu().numpy()
    return embedding[0]


def calculate_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:
    return float(cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0])


def interpret_similarity(score: float) -> str:
    if score >= 0.9: return "ë§¤ìš° ìœ ì‚¬í•¨ (ê±°ì˜ ë™ì¼)"
    elif score >= 0.7: return "ìœ ì‚¬í•¨ (ê°™ì€ ì£¼ì œ)"
    elif score >= 0.5: return "ì–´ëŠ ì •ë„ ê´€ë ¨ ìˆìŒ"
    elif score >= 0.3: return "ì•½ê°„ ê´€ë ¨ ìˆìŒ"
    return "ê´€ë ¨ ì—†ìŒ"


def create_embed_function(model_key: str):
    """ì²­í‚¹ìš© ì„ë² ë”© í•¨ìˆ˜ ìƒì„±"""
    (tokenizer, model), _ = load_model(model_key)
    def embed_fn(text: str) -> np.ndarray:
        return embed_text(text, tokenizer, model)
    return embed_fn


def create_llm_function(llm_model: str, llm_backend: str):
    """ì²­í‚¹ìš© LLM í•¨ìˆ˜ ìƒì„±"""
    def llm_fn(prompt: str) -> str:
        return get_llm_response(
            prompt=prompt,
            llm_model=llm_model,
            llm_backend=llm_backend,
            max_tokens=1024
        )
    return llm_fn


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "Text Similarity + RAG API v5.0 (Extended Chunking + Model Filter)",
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "ollama_available": OllamaLLM.is_available(),
        "features": [
            "RecursiveCharacterTextSplitter",
            "SemanticSplitter",
            "LLM-based Parsing",
            "Embedding Model Filtering (dimâ‰¤1024, memâ‰¤1300MB)"
        ]
    }


@app.get("/models")
def get_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return {
        "preset_embedding_models": PRESET_MODELS,
        "loaded_embedding_models": list(loaded_models.keys()),
        "ollama": {
            "available": OllamaLLM.is_available(),
            "models": OllamaLLM.list_models() if OllamaLLM.is_available() else [],
            "preset_models": OLLAMA_MODELS
        },
        "huggingface_llm_models": HUGGINGFACE_MODELS,
        "device": device
    }


@app.get("/models/llm")
def get_llm_models():
    """LLM ëª¨ë¸ ëª©ë¡ (í”„ë¡ íŠ¸ì—”ë“œìš©)"""
    ollama_available = OllamaLLM.is_available()
    available_ollama_models = OllamaLLM.list_models() if ollama_available else []
    
    ollama_models_with_status = []
    for m in OLLAMA_MODELS:
        ollama_models_with_status.append({
            **m,
            "available": m["key"] in available_ollama_models or any(m["key"].split(":")[0] in a for a in available_ollama_models)
        })
    
    return {
        "ollama": {
            "server_running": ollama_available,
            "available_models": available_ollama_models,
            "models": ollama_models_with_status
        },
        "huggingface": {
            "models": HUGGINGFACE_MODELS
        }
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ API â† NEW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/models/embedding")
def get_embedding_models():
    """
    ì„ë² ë”© ëª¨ë¸ ì „ì²´ ì •ë³´ (í˜¸í™˜/ë¹„í˜¸í™˜ ë¶„ë¥˜)
    - ê¸°ë³¸ í•„í„°: dim â‰¤ 1024, memory â‰¤ 1300MB
    """
    return vector_store.get_embedding_model_info()


@app.post("/models/embedding/filter")
def filter_embedding_models(request: EmbeddingFilterRequest):
    """
    ì»¤ìŠ¤í…€ ì¡°ê±´ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ í•„í„°ë§
    """
    compatible = vector_store.filter_compatible_models(
        max_dim=request.max_dim,
        max_mem=request.max_memory_mb
    )
    return {
        "filter_criteria": {
            "max_dim": request.max_dim,
            "max_memory_mb": request.max_memory_mb
        },
        "compatible_models": compatible,
        "count": len(compatible)
    }


@app.get("/models/embedding/{model_key}/check")
def check_model_compatibility(model_key: str):
    """
    íŠ¹ì • ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬
    """
    model_path = resolve_model_path(model_key)
    is_ok, msg = vector_store.is_model_compatible(model_path)
    spec = vector_store.get_model_spec(model_path)
    
    return {
        "model_key": model_key,
        "model_path": model_path,
        "compatible": is_ok,
        "message": msg,
        "spec": spec
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì²­í‚¹ ë°©ì‹ API â† NEW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/chunking/methods")
def get_chunking_methods():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í‚¹ ë°©ì‹ ëª©ë¡"""
    return {
        "methods": CHUNK_METHODS,
        "default": "article",
        "recommended_order": [
            {"method": "recursive", "desc": "ë­ì²´ì¸ ìŠ¤íƒ€ì¼, ë²”ìš©ì "},
            {"method": "semantic", "desc": "ì˜ë¯¸ ê¸°ë°˜, í’ˆì§ˆ ì¢‹ìŒ (ëŠë¦¼)"},
            {"method": "sentence", "desc": "ë¬¸ì¥ ë‹¨ìœ„, ë¹ ë¦„"},
            {"method": "llm", "desc": "LLM íŒŒì‹±, ê°€ì¥ ì •êµí•¨ (ê°€ì¥ ëŠë¦¼)"},
        ]
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ìŠ¤íŠ¸ ë¹„êµ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/compare")
def compare_texts(request: CompareRequest):
    """ë‘ í…ìŠ¤íŠ¸ ë¹„êµ"""
    (tokenizer, model), load_time = load_model(request.model)
    
    start_time = time.time()
    emb1 = embed_text(request.text1, tokenizer, model)
    emb2 = embed_text(request.text2, tokenizer, model)
    similarity = calculate_similarity(emb1, emb2)
    inference_time = time.time() - start_time
    
    return {
        "similarity": round(similarity, 4),
        "interpretation": interpret_similarity(similarity),
        "model_used": resolve_model_path(request.model),
        "load_time": round(load_time, 2),
        "inference_time": round(inference_time, 4)
    }


@app.post("/compare/models")
def compare_with_multiple_models(request: MultiModelCompareRequest):
    """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì‹œ ë¹„êµ"""
    results = []
    
    for model_key in request.models:
        try:
            (tokenizer, model), load_time = load_model(model_key)
            
            start_time = time.time()
            emb1 = embed_text(request.text1, tokenizer, model)
            emb2 = embed_text(request.text2, tokenizer, model)
            similarity = calculate_similarity(emb1, emb2)
            inference_time = time.time() - start_time
            
            results.append({
                "model_key": model_key,
                "model_path": resolve_model_path(model_key),
                "similarity": round(similarity, 4),
                "interpretation": interpret_similarity(similarity),
                "load_time": round(load_time, 2),
                "inference_time": round(inference_time, 4),
                "success": True,
                "error": None
            })
        except Exception as e:
            results.append({
                "model_key": model_key,
                "model_path": resolve_model_path(model_key),
                "similarity": 0,
                "interpretation": "ë¡œë“œ ì‹¤íŒ¨",
                "success": False,
                "error": str(e)
            })
    
    results.sort(key=lambda x: x["similarity"], reverse=True)
    return {"results": results, "text1": request.text1, "text2": request.text2}


@app.post("/compare/matrix")
def compare_matrix(request: MatrixRequest):
    """ë§¤íŠ¸ë¦­ìŠ¤ ë¹„êµ"""
    (tokenizer, model), _ = load_model(request.model)
    
    embeddings = [embed_text(t, tokenizer, model) for t in request.texts]
    
    n = len(request.texts)
    matrix = [[0.0] * n for _ in range(n)]
    
    for i in range(n):
        for j in range(n):
            matrix[i][j] = round(calculate_similarity(embeddings[i], embeddings[j]), 4)
    
    return {
        "similarity_matrix": matrix,
        "texts": request.texts,
        "model_used": resolve_model_path(request.model)
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG ì—”ë“œí¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form(default="documents"),
    chunk_size: int = Form(default=300),
    chunk_method: str = Form(default="article"),
    overlap: int = Form(default=50),
    model: str = Form(default="ko-sroberta"),
    semantic_threshold: float = Form(default=0.5),
    llm_model: str = Form(default="qwen2.5:3b"),
    llm_backend: str = Form(default="ollama"),
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ë° ì„ë² ë”© ì €ì¥
    
    chunk_method ì˜µì…˜:
    - sentence: ë¬¸ì¥ ë‹¨ìœ„
    - paragraph: ë¬¸ë‹¨ ë‹¨ìœ„
    - article: ì¡°í•­ ë‹¨ìœ„ (SOP/ë²•ë¥ )
    - recursive: RecursiveCharacterTextSplitter (ë­ì²´ì¸)
    - semantic: ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
    - llm: LLM ê¸°ë°˜ êµ¬ì¡° íŒŒì‹±
    """
    content = await file.read()
    filename = file.filename
    model_path = resolve_model_path(model)
    
    # ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬
    is_ok, msg = vector_store.is_model_compatible(model_path)
    if not is_ok:
        raise HTTPException(status_code=400, detail=msg)
    
    text = load_document(filename, content)
    
    if not text.strip():
        raise HTTPException(status_code=400, detail="ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì²­í‚¹ í•¨ìˆ˜ ì¤€ë¹„
    embed_function = None
    llm_function = None
    
    if chunk_method == "semantic":
        embed_function = create_embed_function(model)
    elif chunk_method == "llm":
        llm_function = create_llm_function(llm_model, llm_backend)
    
    # ì²­í‚¹ ìˆ˜í–‰
    chunks = create_chunks(
        text, 
        chunk_size=chunk_size, 
        overlap=overlap, 
        method=chunk_method,
        embed_function=embed_function,
        llm_function=llm_function,
        semantic_threshold=semantic_threshold,
    )
    chunk_texts = [c.text for c in chunks]
    
    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
    metadata_list = []
    for c in chunks:
        meta = {
            "doc_name": filename,
            **c.metadata
        }
        metadata_list.append(meta)
    
    result = vector_store.add_documents(
        chunks=chunk_texts,
        doc_name=filename,
        collection_name=collection,
        model_name=model_path,
        metadata_list=metadata_list
    )
    
    return {
        "success": True,
        "filename": filename,
        "text_length": len(text),
        "chunks_created": len(chunk_texts),
        "chunk_method": chunk_method,
        "chunk_size": chunk_size,
        "collection": collection,
        "model_used": model_path
    }


@app.post("/rag/search")
def search_documents(request: SearchRequest):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    model_path = resolve_model_path(request.model)
    
    results = vector_store.search(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc
    )
    
    return {
        "query": request.query,
        "results": results,
        "count": len(results),
        "model_used": model_path
    }


@app.post("/rag/ask")
def ask_with_agent(request: AskRequest):
    """ì—ì´ì „íŠ¸ íŒ¨í„´ RAG"""
    model_path = resolve_model_path(request.embedding_model)
    
    # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
    results, context = vector_store.search_with_context(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc
    )
    
    if not results:
        return {
            "query": request.query,
            "answer": "ê´€ë ¨ ê·œì • ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "needs_clarification": False
        }
    
    # 2. ë˜ë¬»ê¸° ë¶„ì„
    if request.check_clarification and not request.filter_doc:
        analysis = analyze_search_results(results)
        
        if analysis['needs_clarification']:
            clarification_text = generate_clarification_question(
                query=request.query,
                options=analysis['options'], 
                llm_model=request.llm_model,
                llm_backend=request.llm_backend
            )
            
            return {
                "query": request.query,
                "answer": clarification_text,
                "needs_clarification": True,
                "clarification_options": analysis['options'], 
                "sources": results
            }
    
    # 3. ë‹µë³€ ìƒì„±
    prompt = build_rag_prompt(request.query, context, language="ko")
    
    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=512
        )
    except Exception as e:
        answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    return {
        "query": request.query,
        "answer": answer,
        "needs_clarification": False,
        "sources": results,
        "embedding_model": model_path,
        "llm_model": request.llm_model
    }


@app.post("/rag/ask-llm")
def ask_llm_simple(request: AskRequest):
    """ë‹¨ìˆœ RAG (ë˜ë¬»ê¸° ì—†ì´)"""
    request.check_clarification = False
    return ask_with_agent(request)


@app.post("/rag/ask-chunk")
def ask_with_single_chunk(request: AskChunkRequest):
    """ê°œë³„ ì²­í¬ ê¸°ë°˜ LLM ë‹µë³€"""
    prompt = build_chunk_prompt(request.query, request.chunk_text, language="ko")
    
    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=200
        )
    except Exception as e:
        answer = f"ì˜¤ë¥˜: {str(e)}"
    
    return {
        "query": request.query,
        "answer": answer,
        "llm_model": request.llm_model,
        "llm_backend": request.llm_backend
    }


@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """ë¬¸ì„œ ì‚­ì œ"""
    result = vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )
    return result


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    collection_info = [vector_store.get_collection_info(name) for name in collections]
    return {"collections": collection_info}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    result = vector_store.delete_all(collection_name)
    return result


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› íŒŒì¼ í˜•ì‹"""
    return {"supported_extensions": get_supported_extensions()}


@app.delete("/models/cache")
def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ í´ë¦¬ì–´"""
    global loaded_models
    count = len(loaded_models)
    loaded_models = {}
    torch.cuda.empty_cache()
    return {"message": f"Cleared {count} models from cache"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
        print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
    
    # Ollama ì²´í¬
    if OllamaLLM.is_available():
        models = OllamaLLM.list_models()
        print(f"âœ… Ollama ì„œë²„ ì‹¤í–‰ ì¤‘ ({len(models)}ê°œ ëª¨ë¸)")
    else:
        print("âš ï¸  Ollama ì„œë²„ ë¯¸ì‹¤í–‰ - HuggingFace LLM ì‚¬ìš©")
    
    # ì„ë² ë”© ëª¨ë¸ í•„í„° ì •ë³´
    model_info = vector_store.get_embedding_model_info()
    print(f"\nğŸ“Š ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ (dimâ‰¤1024, memâ‰¤1300MB)")
    print(f"   - í˜¸í™˜: {len(model_info['compatible'])}ê°œ")
    print(f"   - ë¹„í˜¸í™˜: {len(model_info['incompatible'])}ê°œ")
    
    print("=" * 60)
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ + RAG API ì„œë²„ v5.0                         â•‘
    â•‘     (Extended Chunking + Model Filter)                        â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  URL: http://localhost:8000                                   â•‘
    â•‘  Docs: http://localhost:8000/docs                             â•‘
    â•‘                                                               â•‘
    â•‘  ğŸ“„ ì²­í‚¹ ë°©ì‹:                                                â•‘
    â•‘     - sentence: ë¬¸ì¥ ë‹¨ìœ„                                     â•‘
    â•‘     - paragraph: ë¬¸ë‹¨ ë‹¨ìœ„                                    â•‘
    â•‘     - article: ì¡°í•­ ë‹¨ìœ„ (SOP/ë²•ë¥ )                           â•‘
    â•‘     - recursive: RecursiveCharacterTextSplitter               â•‘
    â•‘     - semantic: ì˜ë¯¸ ê¸°ë°˜ (ì„ë² ë”© ìœ ì‚¬ë„)                     â•‘
    â•‘     - llm: LLM ê¸°ë°˜ êµ¬ì¡° íŒŒì‹±                                 â•‘
    â•‘                                                               â•‘
    â•‘  ğŸ” ì„ë² ë”© ëª¨ë¸ í•„í„°ë§:                                       â•‘
    â•‘     GET  /models/embedding         - ì „ì²´ ëª¨ë¸ ì •ë³´           â•‘
    â•‘     POST /models/embedding/filter  - ì»¤ìŠ¤í…€ í•„í„°              â•‘
    â•‘     GET  /models/embedding/{key}/check - í˜¸í™˜ì„± ê²€ì‚¬          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    uvicorn.run(app, host="0.0.0.0", port=8000)