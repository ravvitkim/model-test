"""
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ + RAG API v6.0
- Docling ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹± (í‘œ ì§€ì›)
- ì—ëŸ¬ ìˆ˜ì • (similarity_threshold)
- ê°€ë…ì„± ê°œì„  ë©”íƒ€ë°ì´í„° (ì œNì¡° í˜•ì‹)
- ì—ì´ì „íŠ¸ ì§€ì›
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import torch
import time

# RAG ëª¨ë“ˆ
from rag import (
    load_document,
    get_supported_extensions,
    create_chunks,
    create_chunks_from_blocks,
    get_available_methods,
    CHUNK_METHODS,
)
from rag import vector_store
from rag.prompt import build_rag_prompt, build_chunk_prompt
from rag.llm import (
    get_llm_response,
    OllamaLLM,
    analyze_search_results,
    generate_clarification_question,
    OLLAMA_MODELS,
    HUGGINGFACE_MODELS,
)

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ
from agent import RAGAgent, create_rag_agent, AgentResponse


app = FastAPI(title="RAG API", version="6.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 500
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 5
DEFAULT_SIMILARITY_THRESHOLD = 0.35

PRESET_MODELS = {
    "ko-sroberta": "jhgan/ko-sroberta-multitask",
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5-large": "intfloat/multilingual-e5-large",
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
    "bge-m3": "BAAI/bge-m3",
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
}

device = "cuda" if torch.cuda.is_available() else "cpu"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    check_clarification: bool = True
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class AgentRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    agent_type: str = "basic"  # basic, react, plan_execute
    enable_clarification: bool = True
    filter_doc: Optional[str] = None


class AskChunkRequest(BaseModel):
    query: str
    chunk_text: str
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model_key: str) -> str:
    """í”„ë¦¬ì…‹ í‚¤ â†’ ì‹¤ì œ ê²½ë¡œ"""
    return PRESET_MODELS.get(model_key, model_key)


def format_metadata_display(metadata: Dict) -> Dict:
    """ë©”íƒ€ë°ì´í„° í‘œì‹œ í˜•ì‹ ê°œì„ """
    display = {}

    # ë¬¸ì„œëª…
    if metadata.get('doc_name'):
        display['doc_name'] = metadata['doc_name']

    # ì œëª©
    if metadata.get('doc_title'):
        display['doc_title'] = metadata['doc_title']

    # SOP ID
    if metadata.get('sop_id'):
        display['sop_id'] = metadata['sop_id']

    # ë²„ì „
    if metadata.get('version'):
        display['version'] = f"v{metadata['version']}"

    # ì„¹ì…˜ (ì œNì¡° í˜•ì‹) - ì´ë¯¸ í¬ë§·íŒ…ëœ ê²½ìš°
    if metadata.get('section'):
        display['section'] = metadata['section']
    # ì•„ì§ í¬ë§·íŒ… ì•ˆ ëœ ê²½ìš°
    elif metadata.get('article_num'):
        article_num = metadata['article_num']
        article_type = metadata.get('article_type', 'article')
        if article_type == 'article':
            display['section'] = f"ì œ{article_num}ì¡°"
        elif article_type == 'chapter':
            display['section'] = f"ì œ{article_num}ì¥"
        elif article_type == 'section':
            display['section'] = f"ì œ{article_num}ì ˆ"
        else:
            display['section'] = str(article_num)

    # ì œëª© (ë¸”ë¡)
    if metadata.get('title') and metadata.get('title') != metadata.get('doc_title'):
        display['title'] = metadata['title']

    # í˜ì´ì§€
    if metadata.get('page'):
        display['page'] = f"p.{metadata['page']}"

    return display


def interpret_similarity(score: float) -> str:
    """ìœ ì‚¬ë„ í•´ì„"""
    if score >= 0.85:
        return "ë§¤ìš° ìœ ì‚¬"
    elif score >= 0.65:
        return "ìœ ì‚¬"
    elif score >= 0.50:
        return "ê´€ë ¨ ìˆìŒ"
    elif score >= 0.35:
        return "ì•½ê°„ ê´€ë ¨"
    return "ê´€ë ¨ ë‚®ìŒ"


def interpret_confidence(confidence: str) -> str:
    """ì‹ ë¢°ë„ í•´ì„"""
    return {
        "high": "ğŸŸ¢ ë†’ìŒ",
        "medium": "ğŸŸ¡ ë³´í†µ",
        "low": "ğŸ”´ ë‚®ìŒ",
    }.get(confidence, "âšª ì•Œ ìˆ˜ ì—†ìŒ")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG API v6.0",
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "ollama_available": OllamaLLM.is_available(),
        "features": [
            "Docling ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹± (í‘œ ì§€ì›)",
            "similarity_threshold ê²€ìƒ‰ í•„í„°ë§",
            "ì œNì¡° í˜•ì‹ ë©”íƒ€ë°ì´í„°",
            "ì—ì´ì „íŠ¸ ì§€ì› (basic, react, plan_execute)",
        ],
    }


@app.get("/health")
def health():
    return {"status": "ok", "device": device}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ëª¨ë¸ ì •ë³´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/models/embedding")
def get_embedding_models():
    """ì„ë² ë”© ëª¨ë¸ ì •ë³´"""
    return vector_store.get_embedding_model_info()


@app.get("/models/llm")
def get_llm_models():
    """LLM ëª¨ë¸ ì •ë³´"""
    ollama_running = OllamaLLM.is_available()
    ollama_models = OllamaLLM.list_models() if ollama_running else []

    return {
        "ollama": {
            "server_running": ollama_running,
            "available_models": ollama_models,
            "models": [
                {**m, "available": m["key"] in ollama_models}
                for m in OLLAMA_MODELS
            ],
        },
        "huggingface": {"models": HUGGINGFACE_MODELS},
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ì—…ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    semantic_threshold: float = Form(0.5),
    llm_model: str = Form("qwen2.5:3b"),
    llm_backend: str = Form("ollama"),
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ë²¡í„°í™”"""
    filename = file.filename
    content = await file.read()
    model_path = resolve_model_path(model)

    # ì§€ì› í˜•ì‹ ì²´í¬
    ext = filename.split('.')[-1].lower()
    supported = [e.replace('.', '') for e in get_supported_extensions()]
    if ext not in supported:
        raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: .{ext}")

    try:
        # 1. ë¬¸ì„œ íŒŒì‹±
        parsed_doc = load_document(filename, content)

        # 2. ì²­í‚¹
        if chunk_method == "article" and parsed_doc.blocks:
            chunks = create_chunks_from_blocks(
                parsed_doc,
                chunk_size=chunk_size,
                overlap=overlap,
                method="recursive"
            )
        else:
            # LLM í•¨ìˆ˜ ì¤€ë¹„
            llm_function = None
            if chunk_method == "llm":
                llm_function = lambda p: get_llm_response(p, llm_model, llm_backend, 500)

            # Semanticìš© ì„ë² ë”© í•¨ìˆ˜
            embed_function = None
            if chunk_method == "semantic":
                embed_function = lambda t: vector_store.embed_text(t, model_path)

            chunks = create_chunks(
                parsed_doc.text,
                chunk_size=chunk_size,
                overlap=overlap,
                method=chunk_method,
                embed_function=embed_function,
                llm_function=llm_function,
                semantic_threshold=semantic_threshold,
            )

        if not chunks:
            raise HTTPException(400, "ì²­í¬ ìƒì„± ì‹¤íŒ¨")

        # 3. ë©”íƒ€ë°ì´í„° êµ¬ì„±
        chunk_texts = []
        chunk_metadatas = []

        for chunk in chunks:
            chunk_texts.append(chunk.text)

            meta = {
                "doc_name": filename,
                "doc_title": parsed_doc.metadata.get("title"),
                "sop_id": parsed_doc.metadata.get("sop_id"),
                "version": parsed_doc.metadata.get("version"),
                "chunk_method": chunk_method,
                "chunk_index": chunk.index,
                **chunk.metadata,
            }
            chunk_metadatas.append(meta)

        # 4. ë²¡í„° ì €ì¥
        result = vector_store.add_documents(
            texts=chunk_texts,
            metadatas=chunk_metadatas,
            collection_name=collection,
            model_name=model_path,
        )

        return {
            "success": True,
            "filename": filename,
            "text_length": len(parsed_doc.text),
            "blocks_parsed": len(parsed_doc.blocks),
            "tables_found": len(parsed_doc.tables),
            "chunks_created": len(chunk_texts),
            "chunk_method": chunk_method,
            "collection": collection,
            "model_used": model_path,
            "document_metadata": parsed_doc.metadata,
        }

    except Exception as e:
        raise HTTPException(500, f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/search")
def search_documents(request: SearchRequest):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    model_path = resolve_model_path(request.model)

    results = vector_store.search(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    formatted_results = []
    for r in results:
        formatted_results.append({
            "text": r["text"],
            "similarity": r["similarity"],
            "interpretation": interpret_similarity(r["similarity"]),
            "confidence": r.get("confidence", "medium"),
            "confidence_text": interpret_confidence(r.get("confidence", "medium")),
            "metadata": r["metadata"],
            "metadata_display": format_metadata_display(r["metadata"]),
        })

    # í’ˆì§ˆ ìš”ì•½
    quality_summary = {"message": "ê²°ê³¼ ì—†ìŒ"}
    if formatted_results:
        sims = [r["similarity"] for r in formatted_results]
        quality_summary = {
            "avg_similarity": round(sum(sims) / len(sims), 4),
            "max_similarity": round(max(sims), 4),
            "min_similarity": round(min(sims), 4),
            "high_confidence_count": sum(1 for r in formatted_results if r["confidence"] == "high"),
            "threshold_used": request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD,
        }

    return {
        "query": request.query,
        "results": formatted_results,
        "count": len(formatted_results),
        "model_used": model_path,
        "quality_summary": quality_summary,
    }


@app.post("/rag/search/advanced")
def search_advanced(request: SearchRequest):
    """ê³ ê¸‰ ê²€ìƒ‰"""
    model_path = resolve_model_path(request.model)

    response = vector_store.search_advanced(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )

    return response.to_dict()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - RAG ë‹µë³€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/ask")
def ask_with_rag(request: AskRequest):
    """RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    model_path = resolve_model_path(request.embedding_model)

    # 1. ê²€ìƒ‰
    results, context = vector_store.search_with_context(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )

    if not results:
        return {
            "query": request.query,
            "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "needs_clarification": False,
        }

    # 2. ë˜ë¬»ê¸° ë¶„ì„
    if request.check_clarification and not request.filter_doc:
        analysis = analyze_search_results(results)

        if analysis['needs_clarification']:
            clarification_text = generate_clarification_question(
                query=request.query,
                options=analysis['options'],
                llm_model=request.llm_model,
                llm_backend=request.llm_backend
            )

            return {
                "query": request.query,
                "answer": clarification_text,
                "needs_clarification": True,
                "clarification_options": analysis['options'],
                "sources": [
                    {**r, "metadata_display": format_metadata_display(r.get("metadata", {}))}
                    for r in results
                ],
            }

    # 3. ë‹µë³€ ìƒì„±
    prompt = build_rag_prompt(request.query, context, language="ko")

    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=512
        )
    except Exception as e:
        answer = f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"

    return {
        "query": request.query,
        "answer": answer,
        "needs_clarification": False,
        "sources": [
            {**r, "metadata_display": format_metadata_display(r.get("metadata", {}))}
            for r in results
        ],
        "embedding_model": model_path,
        "llm_model": request.llm_model,
    }


@app.post("/rag/ask-chunk")
def ask_with_chunk(request: AskChunkRequest):
    """ë‹¨ì¼ ì²­í¬ ê¸°ë°˜ ë‹µë³€"""
    prompt = build_chunk_prompt(request.query, request.chunk_text, language="ko")

    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=200
        )
    except Exception as e:
        answer = f"ì˜¤ë¥˜: {str(e)}"

    return {
        "query": request.query,
        "answer": answer,
        "llm_model": request.llm_model,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì—ì´ì „íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/agent")
def run_agent(request: AgentRequest):
    """ì—ì´ì „íŠ¸ ê¸°ë°˜ RAG"""
    model_path = resolve_model_path(request.embedding_model)

    # ê²€ìƒ‰ í•¨ìˆ˜
    def search_fn(query: str, n: int, filter_doc: Optional[str]):
        return vector_store.search_with_context(
            query=query,
            collection_name=request.collection,
            n_results=n,
            model_name=model_path,
            filter_doc=filter_doc,
        )

    # LLM í•¨ìˆ˜
    def llm_fn(prompt: str) -> str:
        return get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=512
        )

    # ë˜ë¬»ê¸° í•¨ìˆ˜
    def clarify_fn(query: str, options: List[Dict]) -> str:
        return generate_clarification_question(
            query=query,
            options=options,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend
        )

    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = create_rag_agent(
        search_fn=search_fn,
        llm_fn=llm_fn,
        analyze_fn=analyze_search_results,
        clarify_fn=clarify_fn,
        agent_type=request.agent_type,
        enable_clarification=request.enable_clarification,
    )

    # ì‹¤í–‰
    try:
        response = agent.run(
            query=request.query,
            n_results=request.n_results,
            filter_doc=request.filter_doc,
        )

        return {
            "query": request.query,
            "answer": response.answer,
            "sources": [
                {**s, "metadata_display": format_metadata_display(s.get("metadata", {}))}
                for s in response.sources
            ],
            "needs_clarification": response.needs_clarification,
            "clarification_options": response.clarification_options,
            "action_taken": response.action_taken,
            "agent_type": request.agent_type,
            "metadata": response.metadata,
        }
    except Exception as e:
        raise HTTPException(500, f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """ë¬¸ì„œ ì‚­ì œ"""
    return vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    return {
        "collections": [
            vector_store.get_collection_info(name)
            for name in collections
        ]
    }


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› íŒŒì¼ í˜•ì‹"""
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²• ëª©ë¡"""
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn

    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  RAG API v6.0")
    print("=" * 60)

    if torch.cuda.is_available():
        print(f"âœ… CUDA: {torch.cuda.get_device_name(0)}")
    else:
        print("âŒ CUDA ë¶ˆê°€ - CPU ëª¨ë“œ")

    if OllamaLLM.is_available():
        models = OllamaLLM.list_models()
        print(f"âœ… Ollama: {len(models)}ê°œ ëª¨ë¸")
    else:
        print("âš ï¸ Ollama ë¯¸ì‹¤í–‰")

    model_info = vector_store.get_embedding_model_info()
    print(f"ğŸ“Š ì„ë² ë”© ëª¨ë¸: í˜¸í™˜ {len(model_info['compatible'])}ê°œ")

    print("=" * 60)
    print("""
    URL: http://localhost:8000
    Docs: http://localhost:8000/docs

    v6.0 ì£¼ìš” ê¸°ëŠ¥:
    - Docling ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹± (í‘œ ì§€ì›)
    - similarity_threshold ê²€ìƒ‰ í•„í„°ë§
    - ì œNì¡° í˜•ì‹ ë©”íƒ€ë°ì´í„°
    - ì—ì´ì „íŠ¸ ì§€ì› (/rag/agent)
    """)

    uvicorn.run(app, host="0.0.0.0", port=8000)