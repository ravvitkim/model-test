"""
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ API - ë¦¬íŒ©í† ë§ v5.1
- document_loader ì§ì ‘ ì‚¬ìš© (ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ ì •ìƒ ì‘ë™)
- ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ì¶”ê°€
- ì²­í¬ í¬ê¸° ê¸°ë³¸ê°’ 300 (í•œêµ­ì–´ ìµœì í™”)
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import time

# RAG ëª¨ë“ˆ (ì •í™•í•œ import)
from rag.document_loader import load_document, get_supported_extensions
from rag.chunker import (
    create_chunks, 
    create_chunks_from_blocks,
    get_available_methods, 
    CHUNK_METHODS,
    Chunk,
)
from rag.parser import ParsedDocument  # ì—¬ê¸°ì„œ import (ì¤‘ë³µ ì •ì˜ ì œê±°!)
from rag import vector_store
from rag.prompt import build_rag_prompt, build_chunk_prompt
from rag.llm import (
    get_llm_response,
    OllamaLLM,
    analyze_search_results,
    generate_clarification_question,
    OLLAMA_MODELS,
    HUGGINGFACE_MODELS,
)


app = FastAPI(title="Text Similarity + RAG API", version="5.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì • ìƒìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì²­í‚¹ ê¸°ë³¸ê°’ (í•œêµ­ì–´ ìµœì í™”)
DEFAULT_CHUNK_SIZE = 300   # ê¸°ì¡´ 500 â†’ 300 (í•œêµ­ì–´ì—ì„œ ë” ì •í™•)
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"  # SOP ë¬¸ì„œìš© ê¸°ë³¸ê°’

# ê²€ìƒ‰ ê¸°ë³¸ê°’
DEFAULT_N_RESULTS = 5
DEFAULT_SIMILARITY_THRESHOLD = 0.35


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í”„ë¦¬ì…‹ ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRESET_MODELS = {
    # í•œêµ­ì–´ ì „ìš© (ê¶Œì¥)
    "ko-sroberta": "jhgan/ko-sroberta-multitask",
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    
    # ë‹¤êµ­ì–´
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5": "intfloat/multilingual-e5-large",
    "bge-m3": "BAAI/bge-m3",
    
    # ì˜ì–´ ì „ìš©
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    
    # Qwen Embedding
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
}

# ì „ì—­ ë³€ìˆ˜
loaded_models = {}
device = "cuda" if torch.cuda.is_available() else "cpu"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CompareRequest(BaseModel):
    text1: str
    text2: str
    model: str = "ko-sroberta"


class MultiModelCompareRequest(BaseModel):
    text1: str
    text2: str
    models: List[str]


class MatrixRequest(BaseModel):
    texts: List[str]
    model: str = "ko-sroberta"


class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "ko-sroberta"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None  # ì¶”ê°€!


class AskRequest(BaseModel):
    """ì—ì´ì „íŠ¸ íŒ¨í„´ RAG ìš”ì²­"""
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "ko-sroberta"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    check_clarification: bool = True
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None  # ì¶”ê°€!


class AskChunkRequest(BaseModel):
    query: str
    chunk_text: str
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"


class EmbeddingFilterRequest(BaseModel):
    max_dim: int = Field(default=1024, description="ìµœëŒ€ ì„ë² ë”© ì°¨ì›")
    max_memory_mb: int = Field(default=1300, description="ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model_key: str) -> str:
    """í”„ë¦¬ì…‹ í‚¤ë©´ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜"""
    return PRESET_MODELS.get(model_key, model_key)


def load_model(model_key: str):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    model_path = resolve_model_path(model_key)
    
    if model_path in loaded_models:
        return loaded_models[model_path], 0.0
    
    print(f"ğŸ“¦ Loading embedding model: {model_path}...")
    start_time = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)
    model.eval()
    
    load_time = time.time() - start_time
    loaded_models[model_path] = (tokenizer, model)
    print(f"âœ… Embedding model loaded: {model_path} ({load_time:.2f}s)")
    
    return (tokenizer, model), load_time


def embed_text(text: str, tokenizer, model) -> np.ndarray:
    """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    MAX_TEXT_LENGTH = 1500
    if len(text) > MAX_TEXT_LENGTH:
        text = text[:MAX_TEXT_LENGTH]
    
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    attention_mask = inputs['attention_mask']
    token_embeddings = outputs.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    embedding = (sum_embeddings / sum_mask).cpu().numpy()
    return embedding[0]


def calculate_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:
    return float(cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0])


def interpret_similarity(score: float) -> str:
    """ìœ ì‚¬ë„ í•´ì„ (ê°œì„ )"""
    if score >= 0.85:
        return "ë§¤ìš° ìœ ì‚¬í•¨ (ê±°ì˜ ë™ì¼)"
    elif score >= 0.65:
        return "ìœ ì‚¬í•¨ (ê°™ì€ ì£¼ì œ, ë†’ì€ ê´€ë ¨ì„±)"
    elif score >= 0.50:
        return "ê´€ë ¨ ìˆìŒ (ë¶€ë¶„ì  ìœ ì‚¬)"
    elif score >= 0.35:
        return "ì•½ê°„ ê´€ë ¨ ìˆìŒ"
    return "ê´€ë ¨ ì—†ìŒ"


def interpret_confidence(confidence: str) -> str:
    """ì‹ ë¢°ë„ í•œê¸€ í•´ì„"""
    return {
        "high": "ğŸŸ¢ ë†’ìŒ (ì‹ ë¢°í•  ìˆ˜ ìˆìŒ)",
        "medium": "ğŸŸ¡ ë³´í†µ (ì°¸ê³ ìš©)",
        "low": "ğŸ”´ ë‚®ìŒ (ê´€ë ¨ì„± ë‚®ì„ ìˆ˜ ìˆìŒ)",
    }.get(confidence, "âšª ì•Œ ìˆ˜ ì—†ìŒ")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "Text Similarity + RAG API v5.1 (Refactored)",
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "ollama_available": OllamaLLM.is_available(),
        "defaults": {
            "chunk_size": DEFAULT_CHUNK_SIZE,
            "chunk_method": DEFAULT_CHUNK_METHOD,
            "similarity_threshold": DEFAULT_SIMILARITY_THRESHOLD,
        },
        "improvements": [
            "ê²€ìƒ‰ ê²°ê³¼ì— ì‹ ë¢°ë„(confidence) í‘œì‹œ",
            "ìœ ì‚¬ë„ threshold í•„í„°ë§",
            "ì²­í¬ í¬ê¸° 300 (í•œêµ­ì–´ ìµœì í™”)",
            "ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ ì •ìƒ ì‘ë™",
        ]
    }


@app.get("/models")
def get_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return {
        "preset_embedding_models": PRESET_MODELS,
        "loaded_embedding_models": list(loaded_models.keys()),
        "ollama": {
            "available": OllamaLLM.is_available(),
            "models": OllamaLLM.list_models() if OllamaLLM.is_available() else [],
            "preset_models": OLLAMA_MODELS
        },
        "huggingface_llm_models": HUGGINGFACE_MODELS,
        "device": device
    }


@app.get("/models/llm")
def get_llm_models():
    """LLM ëª¨ë¸ ëª©ë¡"""
    ollama_available = OllamaLLM.is_available()
    available_ollama_models = OllamaLLM.list_models() if ollama_available else []
    
    ollama_models_with_status = []
    for m in OLLAMA_MODELS:
        ollama_models_with_status.append({
            **m,
            "installed": m["key"] in available_ollama_models
        })
    
    return {
        "ollama": {
            "available": ollama_available,
            "models": ollama_models_with_status,
        },
        "huggingface": HUGGINGFACE_MODELS,
    }


@app.get("/models/embedding")
def get_embedding_models():
    """ì„ë² ë”© ëª¨ë¸ ì •ë³´"""
    return vector_store.get_embedding_model_info()


@app.post("/models/embedding/filter")
def filter_embedding_models(request: EmbeddingFilterRequest):
    """í˜¸í™˜ ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ í•„í„°ë§"""
    compatible = vector_store.filter_compatible_models(
        max_dim=request.max_dim,
        max_mem=request.max_memory_mb
    )
    return {
        "compatible_models": compatible,
        "filter_criteria": {
            "max_dim": request.max_dim,
            "max_memory_mb": request.max_memory_mb
        }
    }


@app.get("/models/embedding/{model_key}/check")
def check_embedding_model(model_key: str):
    """íŠ¹ì • ì„ë² ë”© ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬"""
    model_path = resolve_model_path(model_key)
    is_ok, message = vector_store.is_model_compatible(model_path)
    return {
        "model_key": model_key,
        "model_path": model_path,
        "compatible": is_ok,
        "message": message
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/compare")
def compare_texts(request: CompareRequest):
    """ë‘ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ"""
    model_path = resolve_model_path(request.model)
    (tokenizer, model), load_time = load_model(model_path)
    
    start_time = time.time()
    emb1 = embed_text(request.text1, tokenizer, model)
    emb2 = embed_text(request.text2, tokenizer, model)
    similarity = calculate_similarity(emb1, emb2)
    inference_time = time.time() - start_time
    
    return {
        "similarity": round(similarity, 4),
        "interpretation": interpret_similarity(similarity),
        "model_used": model_path,
        "load_time": round(load_time, 2),
        "inference_time": round(inference_time, 3),
    }


@app.post("/compare/multi")
def compare_multi_model(request: MultiModelCompareRequest):
    """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ìœ ì‚¬ë„ ë¹„êµ"""
    results = []
    
    for model_key in request.models:
        try:
            model_path = resolve_model_path(model_key)
            (tokenizer, model), _ = load_model(model_path)
            
            emb1 = embed_text(request.text1, tokenizer, model)
            emb2 = embed_text(request.text2, tokenizer, model)
            similarity = calculate_similarity(emb1, emb2)
            
            results.append({
                "model": model_key,
                "model_path": model_path,
                "similarity": round(similarity, 4),
                "interpretation": interpret_similarity(similarity),
            })
        except Exception as e:
            results.append({
                "model": model_key,
                "error": str(e)
            })
    
    return {"results": results}


@app.post("/matrix")
def similarity_matrix(request: MatrixRequest):
    """í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ í–‰ë ¬"""
    model_path = resolve_model_path(request.model)
    (tokenizer, model), _ = load_model(model_path)
    
    embeddings = [embed_text(t, tokenizer, model) for t in request.texts]
    emb_array = np.array(embeddings)
    
    matrix = cosine_similarity(emb_array)
    
    return {
        "matrix": matrix.tolist(),
        "texts": request.texts,
        "model_used": model_path
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²• ëª©ë¡"""
    return {
        "methods": CHUNK_METHODS,
        "default": DEFAULT_CHUNK_METHOD,
        "default_chunk_size": DEFAULT_CHUNK_SIZE,
        "recommended_for_korean_sop": "article",
    }


@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    model: str = Form("ko-sroberta"),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    overlap: int = Form(DEFAULT_OVERLAP),
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥ (ìˆ˜ì •ë¨!)
    
    í•µì‹¬ ë³€ê²½: document_loader.load_document() ì§ì ‘ ì‚¬ìš©
    """
    model_path = resolve_model_path(model)
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    filename = file.filename
    supported = get_supported_extensions()
    ext = "." + filename.rsplit(".", 1)[-1].lower() if "." in filename else ""
    if ext not in supported:
        raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}. ì§€ì›: {supported}")
    
    content = await file.read()
    
    # 1ï¸âƒ£ document_loader.load_document() ì‚¬ìš© (í•µì‹¬ ìˆ˜ì •!)
    # ì´ì œ ParsedDocumentê°€ blocksë¥¼ ì œëŒ€ë¡œ í¬í•¨
    parsed_doc = load_document(filename, content)
    
    print(f"ğŸ“„ íŒŒì‹± ì™„ë£Œ: {filename}")
    print(f"   - ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(parsed_doc.text)}")
    print(f"   - ë¸”ë¡ ìˆ˜: {len(parsed_doc.blocks)}")
    
    # 2ï¸âƒ£ ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ (ë¸”ë¡ì´ ìˆëŠ” ê²½ìš°) ë˜ëŠ” ì¼ë°˜ ì²­í‚¹
    if parsed_doc.blocks:
        # ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ (ë©”íƒ€ë°ì´í„° ë³´ì¡´)
        chunks = create_chunks_from_blocks(
            parsed_doc,
            chunk_size=chunk_size,
            overlap=overlap,
            method="recursive" if chunk_method != "article" else "recursive"
        )
        print(f"   - ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹: {len(chunks)}ê°œ ì²­í¬")
    else:
        # ì¼ë°˜ ì²­í‚¹ (ë¸”ë¡ì´ ì—†ëŠ” ê²½ìš°)
        chunks = create_chunks(
            parsed_doc.text,
            chunk_size=chunk_size,
            overlap=overlap,
            method=chunk_method
        )
        print(f"   - ì¼ë°˜ ì²­í‚¹: {len(chunks)}ê°œ ì²­í¬")
    
    # 3ï¸âƒ£ ë©”íƒ€ë°ì´í„° êµ¬ì„±
    chunk_texts = []
    metadata_list = []
    
    for c in chunks:
        chunk_texts.append(c.text)
        
        # Chunk ê°ì²´ì˜ ë©”íƒ€ë°ì´í„° + ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³‘í•©
        meta = {
            "doc_name": filename,
            "doc_title": parsed_doc.metadata.get("title", filename),
            "chunk_method": chunk_method,
            **c.metadata  # Chunkì˜ ë©”íƒ€ë°ì´í„° (article_num, article_type ë“±)
        }
        metadata_list.append(meta)
    
    # 4ï¸âƒ£ ë²¡í„° ì €ì¥
    result = vector_store.add_documents(
        chunks=chunk_texts,
        doc_name=filename,
        collection_name=collection,
        model_name=model_path,
        metadata_list=metadata_list
    )
    
    return {
        "success": True,
        "filename": filename,
        "text_length": len(parsed_doc.text),
        "blocks_parsed": len(parsed_doc.blocks),
        "chunks_created": len(chunk_texts),
        "chunk_method": chunk_method,
        "chunk_size": chunk_size,
        "collection": collection,
        "model_used": model_path,
        "document_metadata": parsed_doc.metadata,
    }


@app.post("/rag/search")
def search_documents(request: SearchRequest):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„ ë¨!)"""
    model_path = resolve_model_path(request.model)
    
    results = vector_store.search(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    
    # ê²°ê³¼ì— í•´ì„ ì¶”ê°€
    for r in results:
        r["interpretation"] = interpret_similarity(r["similarity"])
        r["confidence_text"] = interpret_confidence(r.get("confidence", "medium"))
    
    # í’ˆì§ˆ ìš”ì•½
    if results:
        similarities = [r["similarity"] for r in results]
        quality_summary = {
            "avg_similarity": round(sum(similarities) / len(similarities), 4),
            "max_similarity": round(max(similarities), 4),
            "min_similarity": round(min(similarities), 4),
            "high_confidence_count": sum(1 for r in results if r.get("confidence") == "high"),
            "threshold_used": request.similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD,
        }
    else:
        quality_summary = {"message": "ê²°ê³¼ ì—†ìŒ"}
    
    return {
        "query": request.query,
        "results": results,
        "count": len(results),
        "model_used": model_path,
        "quality_summary": quality_summary,
    }


@app.post("/rag/search/advanced")
def search_advanced(request: SearchRequest):
    """ê³ ê¸‰ ê²€ìƒ‰ (í’ˆì§ˆ ë©”íŠ¸ë¦­ ìƒì„¸)"""
    model_path = resolve_model_path(request.model)
    
    response = vector_store.search_advanced(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    
    return response.to_dict()


@app.post("/rag/ask")
def ask_with_agent(request: AskRequest):
    """ì—ì´ì „íŠ¸ íŒ¨í„´ RAG (ê°œì„ ë¨!)"""
    model_path = resolve_model_path(request.embedding_model)
    
    # 1. ë²¡í„° ê²€ìƒ‰ (threshold ì ìš©)
    results, context = vector_store.search_with_context(
        query=request.query,
        collection_name=request.collection,
        n_results=request.n_results,
        model_name=model_path,
        filter_doc=request.filter_doc,
        similarity_threshold=request.similarity_threshold,
    )
    
    if not results:
        return {
            "query": request.query,
            "answer": "ê´€ë ¨ ê·œì • ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "needs_clarification": False,
            "quality": {"message": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"},
        }
    
    # í’ˆì§ˆ ì²´í¬: ëª¨ë“  ê²°ê³¼ê°€ low confidenceë©´ ê²½ê³ 
    all_low = all(r.get("confidence") == "low" for r in results)
    quality_warning = None
    if all_low:
        quality_warning = "âš ï¸ ê²€ìƒ‰ëœ ëª¨ë“  ê²°ê³¼ì˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."
    
    # 2. ë˜ë¬»ê¸° ë¶„ì„
    if request.check_clarification and not request.filter_doc:
        analysis = analyze_search_results(results)
        
        if analysis['needs_clarification']:
            clarification_text = generate_clarification_question(
                query=request.query,
                options=analysis['options'],
                llm_model=request.llm_model,
                llm_backend=request.llm_backend
            )
            
            return {
                "query": request.query,
                "answer": clarification_text,
                "needs_clarification": True,
                "clarification_options": analysis['options'],
                "sources": results,
                "quality_warning": quality_warning,
            }
    
    # 3. ë‹µë³€ ìƒì„±
    prompt = build_rag_prompt(request.query, context, language="ko")
    
    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=512
        )
    except Exception as e:
        answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # ê²°ê³¼ì— í•´ì„ ì¶”ê°€
    for r in results:
        r["confidence_text"] = interpret_confidence(r.get("confidence", "medium"))
    
    return {
        "query": request.query,
        "answer": answer,
        "needs_clarification": False,
        "sources": results,
        "embedding_model": model_path,
        "llm_model": request.llm_model,
        "quality_warning": quality_warning,
        "quality": {
            "high_confidence_sources": sum(1 for r in results if r.get("confidence") == "high"),
            "total_sources": len(results),
        }
    }


@app.post("/rag/ask-llm")
def ask_llm_simple(request: AskRequest):
    """ë‹¨ìˆœ RAG (ë˜ë¬»ê¸° ì—†ì´)"""
    request.check_clarification = False
    return ask_with_agent(request)


@app.post("/rag/ask-chunk")
def ask_with_single_chunk(request: AskChunkRequest):
    """ê°œë³„ ì²­í¬ ê¸°ë°˜ LLM ë‹µë³€"""
    prompt = build_chunk_prompt(request.query, request.chunk_text, language="ko")
    
    try:
        answer = get_llm_response(
            prompt=prompt,
            llm_model=request.llm_model,
            llm_backend=request.llm_backend,
            max_tokens=200
        )
    except Exception as e:
        answer = f"ì˜¤ë¥˜: {str(e)}"
    
    return {
        "query": request.query,
        "answer": answer,
        "llm_model": request.llm_model,
        "llm_backend": request.llm_backend
    }


@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """ë¬¸ì„œ ì‚­ì œ"""
    result = vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )
    return result


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    collection_info = [vector_store.get_collection_info(name) for name in collections]
    return {"collections": collection_info}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    result = vector_store.delete_all(collection_name)
    return result


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› íŒŒì¼ í˜•ì‹"""
    return {"supported_extensions": get_supported_extensions()}


@app.delete("/models/cache")
def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ í´ë¦¬ì–´"""
    global loaded_models
    count = len(loaded_models)
    loaded_models = {}
    torch.cuda.empty_cache()
    return {"message": f"Cleared {count} models from cache"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
        print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
    
    if OllamaLLM.is_available():
        models = OllamaLLM.list_models()
        print(f"âœ… Ollama ì„œë²„ ì‹¤í–‰ ì¤‘ ({len(models)}ê°œ ëª¨ë¸)")
    else:
        print("âš ï¸  Ollama ì„œë²„ ë¯¸ì‹¤í–‰ - HuggingFace LLM ì‚¬ìš©")
    
    model_info = vector_store.get_embedding_model_info()
    print(f"\nğŸ“Š ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ (dimâ‰¤1024, memâ‰¤1300MB)")
    print(f"   - í˜¸í™˜: {len(model_info['compatible'])}ê°œ")
    print(f"   - ë¹„í˜¸í™˜: {len(model_info['incompatible'])}ê°œ")
    
    print("=" * 60)
    
    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ + RAG API ì„œë²„ v5.1 (Refactored)            â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  URL: http://localhost:8000                                   â•‘
    â•‘  Docs: http://localhost:8000/docs                             â•‘
    â•‘                                                               â•‘
    â•‘  ğŸ“ v5.1 ê°œì„ ì‚¬í•­:                                            â•‘
    â•‘     - ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„(confidence) í‘œì‹œ                       â•‘
    â•‘     - ìœ ì‚¬ë„ threshold í•„í„°ë§ (ê¸°ë³¸ 0.35)                     â•‘
    â•‘     - ì²­í¬ í¬ê¸° 300 (í•œêµ­ì–´ ìµœì í™”)                           â•‘
    â•‘     - ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹ ì •ìƒ ì‘ë™                                â•‘
    â•‘                                                               â•‘
    â•‘  ğŸ” ì™œ 5ê°œ ì¤‘ ì¼ë¶€ë§Œ ê´œì°®ì€ ê²°ê³¼?                             â•‘
    â•‘     â†’ confidence: high/medium/low í™•ì¸í•˜ì„¸ìš”!                 â•‘
    â•‘     â†’ similarity_threshold íŒŒë¼ë¯¸í„°ë¡œ í•„í„°ë§ ê°€ëŠ¥             â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
