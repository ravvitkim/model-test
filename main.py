"""
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ API - ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì›
[ì›ë¬¸] â†’ [íŒŒì‹±: í’ˆì‚¬ ë¶„ì„] â†’ [ì²­í‚¹: ì˜ë¯¸ ë‹¨ìœ„] â†’ [ì„ë² ë”©: ë²¡í„° ë³€í™˜] â†’ [ì½”ì‚¬ì¸ ìœ ì‚¬ë„]
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
import re
import time

app = FastAPI(title="Text Similarity API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í”„ë¦¬ì…‹ ëª¨ë¸ (ë¹ ë¥¸ ì„ íƒìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRESET_MODELS = {
    # í•œêµ­ì–´ ì „ìš©
    "ko-sroberta": "jhgan/ko-sroberta-multitask",
    "ko-sbert": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    "ko-simcse": "BM-K/KoSimCSE-roberta",
    
    # ë‹¤êµ­ì–´
    "multilingual-minilm": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "multilingual-e5": "intfloat/multilingual-e5-large",
    "bge-m3": "BAAI/bge-m3",
    
    # ì˜ì–´ ì „ìš©
    "minilm": "sentence-transformers/all-MiniLM-L6-v2",
    "mpnet": "sentence-transformers/all-mpnet-base-v2",
    
    # Qwen Embedding
    "qwen3-0.6b": "Qwen/Qwen3-Embedding-0.6B",
    "qwen3-4b": "Qwen/Qwen3-Embedding-4B",
}

# ì „ì—­ ë³€ìˆ˜
loaded_models = {}
device = "cuda" if torch.cuda.is_available() else "cpu"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CompareRequest(BaseModel):
    text1: str
    text2: str
    model: str = "ko-sroberta"  # í”„ë¦¬ì…‹ í‚¤ ë˜ëŠ” HuggingFace ëª¨ë¸ ê²½ë¡œ

class MultiModelCompareRequest(BaseModel):
    text1: str
    text2: str
    models: List[str]  # í”„ë¦¬ì…‹ í‚¤ ë˜ëŠ” HuggingFace ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

class MatrixRequest(BaseModel):
    texts: List[str]
    model: str = "ko-sroberta"

class AddModelRequest(BaseModel):
    key: str
    model_path: str

class ProcessedResult(BaseModel):
    original: str
    pos_tags: List[List[str]]
    chunks: List[str]

class CompareResponse(BaseModel):
    similarity: float
    interpretation: str
    text1_processed: ProcessedResult
    text2_processed: ProcessedResult
    model_used: str
    load_time: float
    inference_time: float

class MultiModelResponse(BaseModel):
    results: List[Dict]
    text1: str
    text2: str

class MatrixResponse(BaseModel):
    similarity_matrix: List[List[float]]
    texts: List[str]
    model_used: str

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¨ë¸ ë¡œë”© (ë™ì )
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model_key: str) -> str:
    """í”„ë¦¬ì…‹ í‚¤ë©´ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    return PRESET_MODELS.get(model_key, model_key)

def load_model(model_key: str):
    """ëª¨ë¸ ë¡œë“œ (ìºì‹±) - í”„ë¦¬ì…‹ í‚¤ ë˜ëŠ” ì§ì ‘ HuggingFace ê²½ë¡œ"""
    model_path = resolve_model_path(model_key)
    
    # ì´ë¯¸ ë¡œë“œë¨
    if model_path in loaded_models:
        return loaded_models[model_path], 0.0
    
    print(f"ğŸ“¦ Loading model: {model_path}...")
    start_time = time.time()
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)
        model.eval()
        
        load_time = time.time() - start_time
        loaded_models[model_path] = (tokenizer, model)
        print(f"âœ… Model loaded: {model_path} ({load_time:.2f}s)")
        
        return (tokenizer, model), load_time
    except Exception as e:
        raise ValueError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_path} - {str(e)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_pos(text: str, tokenizer) -> List[List[str]]:
    """Stage 1: í’ˆì‚¬ ë¶„ì„"""
    tokens = tokenizer.tokenize(text)
    pos_tags = []
    
    for token in tokens:
        clean_token = token.replace("##", "").replace("â–", "").replace("Ä ", "")
        if not clean_token:
            continue
        
        if clean_token.isdigit():
            pos = "NUM"
        elif not clean_token.isalnum():
            pos = "PUNCT"
        elif clean_token.isascii() and clean_token.isalpha():
            pos = "WORD_EN"
        else:
            pos = "WORD_KO"
        
        pos_tags.append([clean_token, pos])
    
    return pos_tags

def chunk_text(text: str) -> List[str]:
    """Stage 2: ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹"""
    sentences = re.split(r'(?<=[.!?ã€‚])\s+', text)
    chunks = []
    
    for sentence in sentences:
        if len(sentence) > 100:
            sub_chunks = re.split(r'[,ï¼Œ]|\s+(ê·¸ë¦¬ê³ |ê·¸ëŸ¬ë‚˜|í•˜ì§€ë§Œ|ë˜ëŠ”|ë°|and|but|or)\s+', sentence)
            chunks.extend([c.strip() for c in sub_chunks if c and len(c) > 2])
        else:
            if sentence.strip():
                chunks.append(sentence.strip())
    
    return chunks

def embed_text(text: str, tokenizer, model) -> np.ndarray:
    """Stage 3: ì„ë² ë”© (Mean Pooling)"""
    inputs = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Mean Pooling
    attention_mask = inputs['attention_mask']
    token_embeddings = outputs.last_hidden_state
    
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    embedding = (sum_embeddings / sum_mask).cpu().numpy()
    return embedding[0]

def calculate_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:
    """Stage 4: ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    return float(cosine_similarity(
        emb1.reshape(1, -1),
        emb2.reshape(1, -1)
    )[0][0])

def interpret_similarity(score: float) -> str:
    """ìœ ì‚¬ë„ í•´ì„"""
    if score >= 0.9:
        return "ë§¤ìš° ìœ ì‚¬í•¨ (ê±°ì˜ ë™ì¼)"
    elif score >= 0.7:
        return "ìœ ì‚¬í•¨ (ê°™ì€ ì£¼ì œ)"
    elif score >= 0.5:
        return "ì–´ëŠ ì •ë„ ê´€ë ¨ ìˆìŒ"
    elif score >= 0.3:
        return "ì•½ê°„ ê´€ë ¨ ìˆìŒ"
    else:
        return "ê´€ë ¨ ì—†ìŒ"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "Text Similarity API v2.0",
        "endpoints": ["/compare", "/compare/models", "/compare/matrix", "/models"],
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }

@app.get("/models")
def get_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ëª¨ë¸ + ë¡œë“œëœ ëª¨ë¸"""
    return {
        "preset_models": PRESET_MODELS,
        "loaded_models": list(loaded_models.keys()),
        "device": device,
        "tip": "í”„ë¦¬ì…‹ í‚¤ ë˜ëŠ” HuggingFace ëª¨ë¸ ê²½ë¡œ ì§ì ‘ ì…ë ¥ ê°€ëŠ¥ (ì˜ˆ: Qwen/Qwen3-Embedding-0.6B)"
    }

@app.post("/models/add")
def add_preset_model(request: AddModelRequest):
    """í”„ë¦¬ì…‹ì— ìƒˆ ëª¨ë¸ ì¶”ê°€"""
    PRESET_MODELS[request.key] = request.model_path
    return {"message": f"Added {request.key}: {request.model_path}", "presets": PRESET_MODELS}

@app.post("/compare", response_model=CompareResponse)
def compare_texts(request: CompareRequest):
    """ë‘ í…ìŠ¤íŠ¸ ë¹„êµ - í”„ë¦¬ì…‹ í‚¤ ë˜ëŠ” HuggingFace ê²½ë¡œ ì‚¬ìš© ê°€ëŠ¥"""
    try:
        (tokenizer, model), load_time = load_model(request.model)
        
        start_time = time.time()
        
        # Stage 1: í’ˆì‚¬ ë¶„ì„
        pos1 = parse_pos(request.text1, tokenizer)
        pos2 = parse_pos(request.text2, tokenizer)
        
        # Stage 2: ì²­í‚¹
        chunks1 = chunk_text(request.text1)
        chunks2 = chunk_text(request.text2)
        
        # Stage 3: ì„ë² ë”©
        emb1 = embed_text(request.text1, tokenizer, model)
        emb2 = embed_text(request.text2, tokenizer, model)
        
        # Stage 4: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = calculate_similarity(emb1, emb2)
        
        inference_time = time.time() - start_time
        
        return CompareResponse(
            similarity=round(similarity, 4),
            interpretation=interpret_similarity(similarity),
            text1_processed=ProcessedResult(
                original=request.text1,
                pos_tags=pos1[:10],
                chunks=chunks1
            ),
            text2_processed=ProcessedResult(
                original=request.text2,
                pos_tags=pos2[:10],
                chunks=chunks2
            ),
            model_used=resolve_model_path(request.model),
            load_time=round(load_time, 2),
            inference_time=round(inference_time, 4)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/compare/models", response_model=MultiModelResponse)
def compare_with_multiple_models(request: MultiModelCompareRequest):
    """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì‹œ ë¹„êµ - ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ ì§€ì›"""
    results = []
    
    for model_key in request.models:
        try:
            (tokenizer, model), load_time = load_model(model_key)
            
            start_time = time.time()
            emb1 = embed_text(request.text1, tokenizer, model)
            emb2 = embed_text(request.text2, tokenizer, model)
            similarity = calculate_similarity(emb1, emb2)
            inference_time = time.time() - start_time
            
            results.append({
                "model_key": model_key,
                "model_path": resolve_model_path(model_key),
                "similarity": round(similarity, 4),
                "interpretation": interpret_similarity(similarity),
                "load_time": round(load_time, 2),
                "inference_time": round(inference_time, 4),
                "success": True,
                "error": None
            })
        except Exception as e:
            results.append({
                "model_key": model_key,
                "model_path": resolve_model_path(model_key),
                "similarity": 0,
                "interpretation": "ë¡œë“œ ì‹¤íŒ¨",
                "load_time": 0,
                "inference_time": 0,
                "success": False,
                "error": str(e)
            })
    
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœ ì •ë ¬
    results.sort(key=lambda x: x["similarity"], reverse=True)
    
    return MultiModelResponse(
        results=results,
        text1=request.text1,
        text2=request.text2
    )

@app.post("/compare/matrix", response_model=MatrixResponse)
def compare_matrix(request: MatrixRequest):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤"""
    try:
        (tokenizer, model), _ = load_model(request.model)
        
        embeddings = [embed_text(t, tokenizer, model) for t in request.texts]
        
        n = len(request.texts)
        matrix = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(n):
                matrix[i][j] = round(calculate_similarity(embeddings[i], embeddings[j]), 4)
        
        return MatrixResponse(
            similarity_matrix=matrix,
            texts=request.texts,
            model_used=resolve_model_path(request.model)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/models/cache")
def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ í´ë¦¬ì–´ (ë©”ëª¨ë¦¬ í•´ì œ)"""
    global loaded_models
    count = len(loaded_models)
    loaded_models = {}
    torch.cuda.empty_cache()
    return {"message": f"Cleared {count} models from cache"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    # GPU/CPU ìƒíƒœ ì²´í¬
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
        print(f"   - CUDA ë²„ì „: {torch.version.cuda}")
        print(f"   - PyTorch ë²„ì „: {torch.__version__}")
        print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        print(f"   - PyTorch ë²„ì „: {torch.__version__}")
    
    print(f"\nğŸš€ Device: {device.upper()}")
    print("=" * 60)
    
    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ API ì„œë²„ v2.0                               â•‘
    â•‘  [ì›ë¬¸] â†’ [íŒŒì‹±] â†’ [ì²­í‚¹] â†’ [ì„ë² ë”©] â†’ [ì½”ì‚¬ì¸ ìœ ì‚¬ë„]            â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  URL: http://localhost:8000                                       â•‘
    â•‘  Docs: http://localhost:8000/docs                                 â•‘
    â•‘                                                                   â•‘
    â•‘  ğŸ’¡ ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©ë²•:                                           â•‘
    â•‘     model: "Qwen/Qwen3-Embedding-0.6B"                           â•‘
    â•‘     model: "intfloat/multilingual-e5-small"                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    uvicorn.run(app, host="0.0.0.0", port=8000)